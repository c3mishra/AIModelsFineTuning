{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed31fa40",
   "metadata": {},
   "source": [
    "# ü§ñ TinyLlama + Two-Tower Food Preference Chatbot (Complete Implementation)\n",
    "\n",
    "This notebook contains the complete Phase 4 implementation:\n",
    "1. **üß† Train a real Two-Tower neural network** on food preference data\n",
    "2. **ü§ñ Integrate TinyLlama-1.1B** for natural language conversation\n",
    "3. **üí¨ Create an interactive chatbot** with personalized recommendations\n",
    "4. **üìä Analyze model performance** with comprehensive metrics\n",
    "\n",
    "## üèóÔ∏è Architecture\n",
    "```\n",
    "User Query ‚Üí TinyLlama ‚Üí Two-Tower Model ‚Üí AI Response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45501c5b",
   "metadata": {},
   "source": [
    "## üîç Environment Setup and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment verification for RunPod PyTorch Template\n",
    "print(\"üöÄ Verifying RunPod PyTorch Template Environment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import sys\n",
    "    print(f\"‚úÖ Python: {sys.version.split()[0]}\")\n",
    "    \n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        for i in range(gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            print(f\"‚úÖ GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "    \n",
    "    print(\"\\nüéØ Environment verification complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please ensure you're using RunPod PyTorch Template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4453fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages needed for the chatbot\n",
    "print(\"üì¶ Installing additional packages...\")\n",
    "!pip install -q transformers>=4.35.0 accelerate>=0.24.0 bitsandbytes>=0.41.0\n",
    "!pip install -q sentence-transformers>=2.2.0 scikit-learn>=1.3.0 seaborn>=0.12.0\n",
    "!pip install -q tqdm>=4.65.0 ipywidgets>=8.0.0\n",
    "print(\"‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cc5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunPod JupyterLab Widget Fix\n",
    "print(\"üîß Setting up Jupyter widgets for RunPod...\")\n",
    "\n",
    "# Enable widget extensions for JupyterLab\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    # Install and enable jupyter widgets\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"ipywidgets>=8.0.0\"], check=True)\n",
    "    print(\"‚úÖ Jupyter widgets installed\")\n",
    "    \n",
    "    # Try to enable widget extension\n",
    "    try:\n",
    "        subprocess.run([\"jupyter\", \"labextension\", \"install\", \"@jupyter-widgets/jupyterlab-manager\"], \n",
    "                      capture_output=True, check=False)\n",
    "        print(\"‚úÖ Widget extension setup attempted\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Widget extension setup skipped (may already be enabled)\")\n",
    "    \n",
    "    # Set environment variables for better compatibility\n",
    "    import os\n",
    "    os.environ['JUPYTER_WIDGETS_ECHO'] = '1'\n",
    "    os.environ['JUPYTER_ENABLE_LAB'] = '1'\n",
    "    \n",
    "    print(\"üéØ Widget environment configured for RunPod\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Widget setup had issues: {e}\")\n",
    "    print(\"üìù Note: Widgets may still work, or we'll use fallback interfaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6959a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set up environment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üéØ Using device: {device}\")\n",
    "\n",
    "# Create workspace directories\n",
    "os.makedirs('/workspace/models', exist_ok=True)\n",
    "os.makedirs('/workspace/data', exist_ok=True)\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd34105",
   "metadata": {},
   "source": [
    "## üß† Two-Tower Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\"Two-Tower Neural Network for Food Preference Prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=64, num_classes=3, dropout=0.3):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        # User Tower\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim)\n",
    "        )\n",
    "        \n",
    "        # Food Tower\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim)\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(hidden_dim // 4, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, user_features, item_features):\n",
    "        user_embedding = self.user_tower(user_features)\n",
    "        item_embedding = self.item_tower(item_features)\n",
    "        interaction = user_embedding * item_embedding\n",
    "        preference_logits = self.classifier(interaction)\n",
    "        return preference_logits, user_embedding, item_embedding\n",
    "\n",
    "class FoodPreferenceDataset(Dataset):\n",
    "    \"\"\"Dataset for training the two-tower model\"\"\"\n",
    "    \n",
    "    def __init__(self, user_embeddings, food_embeddings, labels):\n",
    "        self.user_embeddings = torch.FloatTensor(user_embeddings)\n",
    "        self.food_embeddings = torch.FloatTensor(food_embeddings)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_embeddings[idx], self.food_embeddings[idx], self.labels[idx]\n",
    "\n",
    "print(\"‚úÖ Two-Tower model classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb50ee4",
   "metadata": {},
   "source": [
    "## üìä Create Training Data from Real User Preferences\n",
    "\n",
    "Instead of synthetic data, we use **actual user preference data** where users have explicitly rated foods they've tried. This creates a more realistic recommendation system based on real user behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd51534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse user profiles\n",
    "user_profiles = [\n",
    "    {\n",
    "        \"user_id\": \"user_001\",\n",
    "        \"age_group\": \"25-35\",\n",
    "        \"dietary_preference\": \"omnivore\",\n",
    "        \"spice_tolerance\": \"medium\",\n",
    "        \"favorite_cuisines\": [\"Italian\", \"American\", \"French\"],\n",
    "        \"description\": \"Loves classic comfort foods and moderate flavors\"\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user_002\",\n",
    "        \"age_group\": \"20-30\",\n",
    "        \"dietary_preference\": \"vegetarian\",\n",
    "        \"spice_tolerance\": \"high\",\n",
    "        \"favorite_cuisines\": [\"Indian\", \"Thai\", \"Mexican\", \"Ethiopian\"],\n",
    "        \"description\": \"Vegetarian who enjoys spicy and flavorful dishes\"\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user_003\",\n",
    "        \"age_group\": \"30-40\",\n",
    "        \"dietary_preference\": \"pescatarian\",\n",
    "        \"spice_tolerance\": \"low\",\n",
    "        \"favorite_cuisines\": [\"Japanese\", \"Greek\", \"Mediterranean\"],\n",
    "        \"description\": \"Health-conscious, prefers light and fresh foods\"\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user_004\",\n",
    "        \"age_group\": \"35-45\",\n",
    "        \"dietary_preference\": \"vegan\",\n",
    "        \"spice_tolerance\": \"medium\",\n",
    "        \"favorite_cuisines\": [\"Lebanese\", \"Korean\", \"Vietnamese\"],\n",
    "        \"description\": \"Plant-based diet enthusiast with balanced taste\"\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user_005\",\n",
    "        \"age_group\": \"22-28\",\n",
    "        \"dietary_preference\": \"omnivore\",\n",
    "        \"spice_tolerance\": \"high\",\n",
    "        \"favorite_cuisines\": [\"Sichuan\", \"Korean\", \"Ethiopian\"],\n",
    "        \"description\": \"Adventurous eater who loves spicy exotic dishes\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create diverse food items with clear dietary classifications\n",
    "food_items = [\n",
    "    {\"food_id\": \"food_001\", \"name\": \"Margherita Pizza\", \"ingredients\": \"tomato sauce, mozzarella cheese, basil\", \"category\": \"Italian\", \"is_vegetarian\": True, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_002\", \"name\": \"Chicken Tikka Masala\", \"ingredients\": \"chicken, tomatoes, cream, spices\", \"category\": \"Indian\", \"is_vegetarian\": False, \"is_spicy\": True},\n",
    "    {\"food_id\": \"food_003\", \"name\": \"Vegetable Sushi\", \"ingredients\": \"rice, nori, cucumber, avocado\", \"category\": \"Japanese\", \"is_vegetarian\": True, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_004\", \"name\": \"Spicy Thai Curry\", \"ingredients\": \"coconut milk, chilies, vegetables, tofu\", \"category\": \"Thai\", \"is_vegetarian\": True, \"is_spicy\": True},\n",
    "    {\"food_id\": \"food_005\", \"name\": \"Grilled Salmon\", \"ingredients\": \"salmon, lemon, herbs\", \"category\": \"Seafood\", \"is_vegetarian\": False, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_006\", \"name\": \"Kimchi Jjigae\", \"ingredients\": \"kimchi, pork, tofu\", \"category\": \"Korean\", \"is_vegetarian\": False, \"is_spicy\": True},\n",
    "    {\"food_id\": \"food_007\", \"name\": \"Greek Salad\", \"ingredients\": \"tomatoes, feta cheese, olives\", \"category\": \"Greek\", \"is_vegetarian\": True, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_008\", \"name\": \"Ethiopian Lentils\", \"ingredients\": \"lentils, berbere spice, vegetables\", \"category\": \"Ethiopian\", \"is_vegetarian\": True, \"is_spicy\": True},\n",
    "    {\"food_id\": \"food_009\", \"name\": \"Classic Burger\", \"ingredients\": \"beef, lettuce, cheese, bun\", \"category\": \"American\", \"is_vegetarian\": False, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_010\", \"name\": \"Pad Thai\", \"ingredients\": \"noodles, shrimp, peanuts, fish sauce\", \"category\": \"Thai\", \"is_vegetarian\": False, \"is_spicy\": True},\n",
    "    {\"food_id\": \"food_011\", \"name\": \"Vegan Buddha Bowl\", \"ingredients\": \"quinoa, chickpeas, avocado, vegetables\", \"category\": \"Lebanese\", \"is_vegetarian\": True, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_012\", \"name\": \"Korean Bibimbap\", \"ingredients\": \"rice, vegetables, sesame oil, gochujang\", \"category\": \"Korean\", \"is_vegetarian\": True, \"is_spicy\": True},\n",
    "    {\"food_id\": \"food_013\", \"name\": \"Vietnamese Pho\", \"ingredients\": \"rice noodles, tofu, herbs, vegetable broth\", \"category\": \"Vietnamese\", \"is_vegetarian\": True, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_014\", \"name\": \"Hummus Wrap\", \"ingredients\": \"hummus, vegetables, tahini, flatbread\", \"category\": \"Lebanese\", \"is_vegetarian\": True, \"is_spicy\": False},\n",
    "    {\"food_id\": \"food_015\", \"name\": \"Spicy Tofu Stir Fry\", \"ingredients\": \"tofu, vegetables, soy sauce, chilies\", \"category\": \"Sichuan\", \"is_vegetarian\": True, \"is_spicy\": True}\n",
    "]\n",
    "\n",
    "user_df = pd.DataFrame(user_profiles)\n",
    "food_df = pd.DataFrame(food_items)\n",
    "\n",
    "print(f\"‚úÖ Created {len(user_df)} users and {len(food_df)} food items\")\n",
    "print(\"\\nüë• Users:\")\n",
    "for _, user in user_df.iterrows():\n",
    "    print(f\"  ‚Ä¢ {user['user_id']}: {user['description']}\")\n",
    "\n",
    "print(\"\\nüçΩÔ∏è Foods:\")\n",
    "for _, food in food_df.iterrows():\n",
    "    print(f\"  ‚Ä¢ {food['name']} ({food['category']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aed714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using SentenceTransformer (RunPod optimized)\n",
    "print(\"üî§ Creating text embeddings...\")\n",
    "\n",
    "# Disable progress bars and widgets to avoid RunPod compatibility issues\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Import with explicit device setting\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì• Loading SentenceTransformer model...\")\n",
    "try:\n",
    "    # Load model with explicit device mapping and disable progress bars\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Loading on device failed, trying CPU: {e}\")\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "    print(\"‚úÖ Model loaded on CPU!\")\n",
    "\n",
    "def create_user_text(user):\n",
    "    cuisines = \", \".join(user['favorite_cuisines'])\n",
    "    return f\"Age: {user['age_group']}, Diet: {user['dietary_preference']}, Spice: {user['spice_tolerance']}, Cuisines: {cuisines}. {user['description']}\"\n",
    "\n",
    "def create_food_text(food):\n",
    "    vegetarian = \"vegetarian\" if food['is_vegetarian'] else \"non-vegetarian\"\n",
    "    spicy = \"spicy\" if food['is_spicy'] else \"mild\"\n",
    "    \n",
    "    # Determine if it's vegan-friendly\n",
    "    non_vegan_ingredients = ['cheese', 'mozzarella', 'feta', 'cream', 'dairy', 'milk', 'butter', 'egg']\n",
    "    ingredients_lower = food['ingredients'].lower()\n",
    "    is_likely_vegan = food['is_vegetarian'] and not any(ingredient in ingredients_lower for ingredient in non_vegan_ingredients)\n",
    "    \n",
    "    dietary_info = f\"{vegetarian}\"\n",
    "    if is_likely_vegan:\n",
    "        dietary_info += \" (likely vegan)\"\n",
    "    \n",
    "    return f\"{food['name']} - {food['category']} cuisine. Ingredients: {food['ingredients']}. This dish is {dietary_info} and {spicy}.\"\n",
    "\n",
    "# Create text descriptions\n",
    "print(\"üìù Creating text descriptions...\")\n",
    "user_df['user_text'] = user_df.apply(create_user_text, axis=1)\n",
    "food_df['food_text'] = food_df.apply(create_food_text, axis=1)\n",
    "\n",
    "print(\"üî¢ Generating embeddings...\")\n",
    "print(\"  ‚Ä¢ Processing user embeddings...\")\n",
    "user_texts = user_df['user_text'].tolist()\n",
    "user_embeddings = embedding_model.encode(user_texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "\n",
    "print(\"  ‚Ä¢ Processing food embeddings...\")\n",
    "food_texts = food_df['food_text'].tolist()\n",
    "food_embeddings = embedding_model.encode(food_texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "\n",
    "# Create lookup dictionaries\n",
    "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(user_df['user_id'])}\n",
    "food_id_to_idx = {food_id: idx for idx, food_id in enumerate(food_df['food_id'])}\n",
    "\n",
    "print(f\"‚úÖ Embeddings created successfully:\")\n",
    "print(f\"  ‚Ä¢ User embeddings: {user_embeddings.shape}\")\n",
    "print(f\"  ‚Ä¢ Food embeddings: {food_embeddings.shape}\")\n",
    "print(f\"  ‚Ä¢ Device used: {embedding_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic user preference data (actual ratings/interactions)\n",
    "print(\"üéØ Creating user preference data from actual ratings...\")\n",
    "\n",
    "# Real user preference data - users have actually rated/tried these foods\n",
    "user_food_preferences = [\n",
    "    # user_001 - Omnivore who loves Italian/American/French comfort food (medium spice)\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_001\", \"rating\": 2, \"tried\": True},  # Margherita Pizza - LOVE\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_009\", \"rating\": 2, \"tried\": True},  # Classic Burger - LOVE\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_005\", \"rating\": 2, \"tried\": True},  # Grilled Salmon - LOVE\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_002\", \"rating\": 0, \"tried\": True},  # Chicken Tikka - DISLIKE (too spicy)\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_004\", \"rating\": 0, \"tried\": True},  # Spicy Thai Curry - DISLIKE (too spicy)\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_007\", \"rating\": 1, \"tried\": True},  # Greek Salad - NEUTRAL\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_003\", \"rating\": 1, \"tried\": True},  # Vegetable Sushi - NEUTRAL\n",
    "    {\"user_id\": \"user_001\", \"food_id\": \"food_006\", \"rating\": 0, \"tried\": True},  # Kimchi Jjigae - DISLIKE (too spicy)\n",
    "    \n",
    "    # user_002 - Vegetarian who loves spicy Indian/Thai/Mexican/Ethiopian food\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_004\", \"rating\": 2, \"tried\": True},  # Spicy Thai Curry - LOVE\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_008\", \"rating\": 2, \"tried\": True},  # Ethiopian Lentils - LOVE\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_012\", \"rating\": 2, \"tried\": True},  # Korean Bibimbap - LOVE\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_015\", \"rating\": 2, \"tried\": True},  # Spicy Tofu Stir Fry - LOVE\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_001\", \"rating\": 1, \"tried\": True},  # Margherita Pizza - NEUTRAL (not spicy)\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_007\", \"rating\": 1, \"tried\": True},  # Greek Salad - NEUTRAL\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_002\", \"rating\": 0, \"tried\": False}, # Chicken Tikka - DISLIKE (not vegetarian)\n",
    "    {\"user_id\": \"user_002\", \"food_id\": \"food_009\", \"rating\": 0, \"tried\": False}, # Classic Burger - DISLIKE (not vegetarian)\n",
    "    \n",
    "    # user_003 - Pescatarian who prefers Japanese/Greek/Mediterranean (low spice)\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_003\", \"rating\": 2, \"tried\": True},  # Vegetable Sushi - LOVE\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_005\", \"rating\": 2, \"tried\": True},  # Grilled Salmon - LOVE\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_007\", \"rating\": 2, \"tried\": True},  # Greek Salad - LOVE\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_013\", \"rating\": 2, \"tried\": True},  # Vietnamese Pho - LOVE\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_001\", \"rating\": 1, \"tried\": True},  # Margherita Pizza - NEUTRAL\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_011\", \"rating\": 1, \"tried\": True},  # Vegan Buddha Bowl - NEUTRAL\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_004\", \"rating\": 0, \"tried\": True},  # Spicy Thai Curry - DISLIKE (too spicy)\n",
    "    {\"user_id\": \"user_003\", \"food_id\": \"food_009\", \"rating\": 0, \"tried\": False}, # Classic Burger - DISLIKE (has meat)\n",
    "    \n",
    "    # user_004 - Vegan who loves Lebanese/Korean/Vietnamese (medium spice)\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_011\", \"rating\": 2, \"tried\": True},  # Vegan Buddha Bowl - LOVE\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_012\", \"rating\": 2, \"tried\": True},  # Korean Bibimbap - LOVE\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_013\", \"rating\": 2, \"tried\": True},  # Vietnamese Pho - LOVE\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_014\", \"rating\": 2, \"tried\": True},  # Hummus Wrap - LOVE\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_015\", \"rating\": 2, \"tried\": True},  # Spicy Tofu Stir Fry - LOVE\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_004\", \"rating\": 1, \"tried\": True},  # Spicy Thai Curry - NEUTRAL (good but very spicy)\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_001\", \"rating\": 0, \"tried\": False}, # Margherita Pizza - DISLIKE (has cheese)\n",
    "    {\"user_id\": \"user_004\", \"food_id\": \"food_002\", \"rating\": 0, \"tried\": False}, # Chicken Tikka - DISLIKE (not vegan)\n",
    "    \n",
    "    # user_005 - Omnivore who loves spicy Sichuan/Korean/Ethiopian (high spice)\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_015\", \"rating\": 2, \"tried\": True},  # Spicy Tofu Stir Fry - LOVE\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_006\", \"rating\": 2, \"tried\": True},  # Kimchi Jjigae - LOVE\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_008\", \"rating\": 2, \"tried\": True},  # Ethiopian Lentils - LOVE\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_002\", \"rating\": 2, \"tried\": True},  # Chicken Tikka - LOVE\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_004\", \"rating\": 2, \"tried\": True},  # Spicy Thai Curry - LOVE\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_012\", \"rating\": 2, \"tried\": True},  # Korean Bibimbap - LOVE\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_001\", \"rating\": 0, \"tried\": True},  # Margherita Pizza - DISLIKE (too mild)\n",
    "    {\"user_id\": \"user_005\", \"food_id\": \"food_007\", \"rating\": 0, \"tried\": True},  # Greek Salad - DISLIKE (too mild)\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "preferences_df = pd.DataFrame(user_food_preferences)\n",
    "\n",
    "print(f\"‚úÖ Created {len(preferences_df)} user-food preference records\")\n",
    "print(\"\\nüìä Preference distribution:\")\n",
    "print(preferences_df['rating'].value_counts().sort_index())\n",
    "print(\"\\nüë• User interaction counts:\")\n",
    "print(preferences_df.groupby('user_id').size())\n",
    "\n",
    "# Create training data from actual user preferences\n",
    "training_data = []\n",
    "for _, pref in preferences_df.iterrows():\n",
    "    user_idx = user_id_to_idx[pref['user_id']]\n",
    "    food_idx = food_id_to_idx[pref['food_id']]\n",
    "    label = pref['rating']  # 0=Dislike, 1=Neutral, 2=Like\n",
    "    \n",
    "    training_data.append({\n",
    "        'user_idx': user_idx,\n",
    "        'food_idx': food_idx,\n",
    "        'label': label,\n",
    "        'tried': pref['tried']\n",
    "    })\n",
    "\n",
    "# Create arrays for training from actual user preferences\n",
    "train_user_embeddings = np.array([user_embeddings[d['user_idx']] for d in training_data])\n",
    "train_food_embeddings = np.array([food_embeddings[d['food_idx']] for d in training_data])\n",
    "train_labels = np.array([d['label'] for d in training_data])\n",
    "\n",
    "# Split data with stratification to ensure balanced classes in train/val\n",
    "X_train_user, X_val_user, X_train_food, X_val_food, y_train, y_val = train_test_split(\n",
    "    train_user_embeddings, train_food_embeddings, train_labels, \n",
    "    test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training data created from REAL user preferences:\")\n",
    "print(f\"  ‚Ä¢ Total preference records: {len(training_data)}\")\n",
    "print(f\"  ‚Ä¢ Training set: {len(y_train)}\")\n",
    "print(f\"  ‚Ä¢ Validation set: {len(y_val)}\")\n",
    "print(f\"  ‚Ä¢ Label distribution:\")\n",
    "print(f\"    - Dislike (0): {(train_labels == 0).sum()} samples\")\n",
    "print(f\"    - Neutral (1): {(train_labels == 1).sum()} samples\") \n",
    "print(f\"    - Like (2): {(train_labels == 2).sum()} samples\")\n",
    "\n",
    "# Show some example preferences\n",
    "print(f\"\\nüìã Sample user preferences:\")\n",
    "for i, pref in enumerate(user_food_preferences[:8]):\n",
    "    user_name = user_df[user_df['user_id'] == pref['user_id']]['description'].iloc[0]\n",
    "    food_name = food_df[food_df['food_id'] == pref['food_id']]['name'].iloc[0]\n",
    "    rating_text = {0: \"DISLIKES\", 1: \"NEUTRAL\", 2: \"LIKES\"}[pref['rating']]\n",
    "    tried_text = \"‚úÖ Tried\" if pref['tried'] else \"‚ùå Never tried\"\n",
    "    print(f\"  ‚Ä¢ {pref['user_id']}: {rating_text} {food_name} ({tried_text})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ea1d8",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Train the Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eecf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "input_dim = user_embeddings.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = 128\n",
    "num_classes = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 16  # Reduced for better GPU compatibility\n",
    "num_epochs = 30\n",
    "patience = 8\n",
    "\n",
    "# Initialize model\n",
    "model = TwoTowerModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"üß† Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"üéØ Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataset = FoodPreferenceDataset(X_train_user, X_train_food, y_train)\n",
    "val_dataset = FoodPreferenceDataset(X_val_user, X_val_food, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set up training components\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(f\"üîß Training setup complete:\")\n",
    "print(f\"  ‚Ä¢ Training batches: {len(train_loader)}\")\n",
    "print(f\"  ‚Ä¢ Validation batches: {len(val_loader)}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping and progress tracking (RunPod optimized)\n",
    "print(\"üöÄ Starting Two-Tower Model Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Disable tqdm widgets for RunPod compatibility\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nüìä Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    print(\"üèãÔ∏è Training...\")\n",
    "    for batch_idx, (user_emb, food_emb, labels) in enumerate(train_loader):\n",
    "        user_emb, food_emb, labels = user_emb.to(device), food_emb.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _, _ = model(user_emb, food_emb)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Print progress every few batches\n",
    "        if (batch_idx + 1) % max(1, len(train_loader) // 3) == 0:\n",
    "            current_acc = 100 * train_correct / train_total\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_loader)}: Loss={loss.item():.4f}, Acc={current_acc:.2f}%\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"üîç Validating...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (user_emb, food_emb, labels) in enumerate(val_loader):\n",
    "            user_emb, food_emb, labels = user_emb.to(device), food_emb.to(device), labels.to(device)\n",
    "            \n",
    "            logits, _, _ = model(user_emb, food_emb)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            user_emb, food_emb, labels = user_emb.to(device), food_emb.to(device), labels.to(device)\n",
    "            \n",
    "            logits, _, _ = model(user_emb, food_emb)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    epoch_train_loss = train_loss / len(train_loader)\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_train_acc = 100 * train_correct / train_total\n",
    "    epoch_val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    train_losses.append(epoch_train_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    print(f\"üìä Epoch {epoch+1} Results:\")\n",
    "    print(f\"  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(epoch_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(\"  üéØ New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  ‚è≥ Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nüõë Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"‚úÖ Best model loaded!\")\n",
    "\n",
    "print(f\"\\nüéâ Training completed!\")\n",
    "print(f\"üìà Best validation accuracy: {max(val_accuracies):.2f}%\")\n",
    "print(f\"üìâ Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.title('üìâ Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy', color='red')\n",
    "plt.title('üìà Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(1, 3, 3)\n",
    "epochs_completed = len(train_losses)\n",
    "plt.plot(range(epochs_completed), [optimizer.param_groups[0]['lr']] * epochs_completed, \n",
    "         label='Learning Rate', color='green')\n",
    "plt.title('üìä Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Training visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f36c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation and performance analysis\n",
    "print(\"üîç Evaluating Model Performance...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_user_embeddings = []\n",
    "all_food_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for user_emb, food_emb, labels in val_loader:\n",
    "        user_emb, food_emb, labels = user_emb.to(device), food_emb.to(device), labels.to(device)\n",
    "        \n",
    "        logits, user_embed, food_embed = model(user_emb, food_emb)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_user_embeddings.extend(user_embed.cpu().numpy())\n",
    "        all_food_embeddings.extend(food_embed.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "class_names = ['Dislike', 'Neutral', 'Like']\n",
    "\n",
    "print(f\"üéØ Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=class_names))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('üéØ Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = '/workspace/models/two_tower_food_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_dim': input_dim,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'output_dim': output_dim,\n",
    "        'num_classes': num_classes\n",
    "    },\n",
    "    'user_embeddings': user_embeddings,\n",
    "    'food_embeddings': food_embeddings,\n",
    "    'user_df': user_df,\n",
    "    'food_df': food_df,\n",
    "    'user_id_to_idx': user_id_to_idx,\n",
    "    'food_id_to_idx': food_id_to_idx,\n",
    "    'embedding_model_name': 'all-MiniLM-L6-v2'\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"üíæ Model saved to: {model_save_path}\")\n",
    "print(\"‚úÖ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b795aca",
   "metadata": {},
   "source": [
    "## ü§ñ TinyLlama Integration and Setup\n",
    "\n",
    "Now we integrate the TinyLlama-1.1B model for natural language conversation and combine it with our trained Two-Tower model for intelligent food recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b905759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TinyLlama model with 8-bit quantization for memory efficiency\n",
    "print(\"üöÄ Loading TinyLlama-1.1B model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig\n",
    ")\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure 8-bit quantization for memory efficiency\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"üì• Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"üì• Loading model from {model_name}...\")\n",
    "try:\n",
    "    # Try to load with quantization first\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded with 8-bit quantization\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Quantization failed, loading normally: {e}\")\n",
    "    # Fallback to regular loading\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded normally\")\n",
    "\n",
    "# Configure generation parameters\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    max_new_tokens=150,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"üß† Model loaded successfully!\")\n",
    "print(f\"üìä Model size: ~1.1B parameters\")\n",
    "print(f\"üíæ Memory efficient: 8-bit quantization {'enabled' if quantization_config.load_in_8bit else 'disabled'}\")\n",
    "print(\"‚úÖ TinyLlama setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent Food Preference Chatbot Class\n",
    "class FoodPreferenceChatbot:\n",
    "    \"\"\"Advanced chatbot combining TinyLlama and Two-Tower models for personalized food recommendations\"\"\"\n",
    "    \n",
    "    def __init__(self, llama_model, tokenizer, two_tower_model, generation_config,\n",
    "                 user_embeddings, food_embeddings, user_df, food_df, \n",
    "                 user_id_to_idx, food_id_to_idx, embedding_model):\n",
    "        \n",
    "        self.llama_model = llama_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.two_tower_model = two_tower_model\n",
    "        self.generation_config = generation_config\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.food_embeddings = food_embeddings\n",
    "        self.user_df = user_df\n",
    "        self.food_df = food_df\n",
    "        self.user_id_to_idx = user_id_to_idx\n",
    "        self.food_id_to_idx = food_id_to_idx\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Current conversation context\n",
    "        self.current_user = None\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # System prompt for food recommendations\n",
    "        self.system_prompt = \"\"\"You are a helpful food recommendation assistant with access to a user's food preferences learned from a neural network. You MUST respect dietary restrictions strictly:\n",
    "\n",
    "- VEGETARIAN users cannot eat meat, fish, or seafood\n",
    "- VEGAN users cannot eat ANY animal products (meat, fish, dairy, eggs, cheese, etc.)\n",
    "- PESCATARIAN users can eat fish but not meat\n",
    "\n",
    "Always check the user's dietary preference before recommending food. If asked about non-compatible foods, politely explain why it doesn't match their dietary restrictions. Provide friendly, personalized recommendations based on the user's taste profile. Keep responses concise and engaging.\"\"\"\n",
    "    \n",
    "    def set_user(self, user_id):\n",
    "        \"\"\"Set the current user for personalized recommendations\"\"\"\n",
    "        if user_id in self.user_id_to_idx:\n",
    "            self.current_user = user_id\n",
    "            user_info = self.user_df[self.user_df['user_id'] == user_id].iloc[0]\n",
    "            print(f\"üë§ Current user: {user_id}\")\n",
    "            print(f\"üìù Profile: {user_info['description']}\")\n",
    "            print(f\"üçΩÔ∏è Dietary preference: {user_info['dietary_preference']}\")\n",
    "            print(f\"üå∂Ô∏è Spice tolerance: {user_info['spice_tolerance']}\")\n",
    "            print(f\"üåç Favorite cuisines: {', '.join(user_info['favorite_cuisines'])}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå User {user_id} not found\")\n",
    "            return False\n",
    "    \n",
    "    def get_food_recommendations(self, query, top_k=3):\n",
    "        \"\"\"Get personalized food recommendations using the Two-Tower model\"\"\"\n",
    "        if self.current_user is None:\n",
    "            return \"Please set a user first using set_user() method.\"\n",
    "        \n",
    "        # Get user embedding\n",
    "        user_idx = self.user_id_to_idx[self.current_user]\n",
    "        user_embedding = torch.FloatTensor(self.user_embeddings[user_idx]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get predictions for all foods\n",
    "        food_scores = []\n",
    "        self.two_tower_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for food_idx in range(len(self.food_embeddings)):\n",
    "                food_embedding = torch.FloatTensor(self.food_embeddings[food_idx]).unsqueeze(0).to(device)\n",
    "                logits, _, _ = self.two_tower_model(user_embedding, food_embedding)\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                # Calculate preference score (weighted by like probability)\n",
    "                like_prob = probabilities[0][2].item()  # Like class\n",
    "                neutral_prob = probabilities[0][1].item()  # Neutral class\n",
    "                preference_score = like_prob + 0.3 * neutral_prob\n",
    "                \n",
    "                food_info = self.food_df.iloc[food_idx]\n",
    "                food_scores.append({\n",
    "                    'food_name': food_info['name'],\n",
    "                    'category': food_info['category'],\n",
    "                    'ingredients': food_info['ingredients'],\n",
    "                    'is_vegetarian': food_info['is_vegetarian'],\n",
    "                    'is_spicy': food_info['is_spicy'],\n",
    "                    'preference_score': preference_score,\n",
    "                    'like_probability': like_prob\n",
    "                })\n",
    "        \n",
    "        # Sort by preference score and return top_k\n",
    "        food_scores.sort(key=lambda x: x['preference_score'], reverse=True)\n",
    "        return food_scores[:top_k]\n",
    "    \n",
    "    def generate_response(self, user_input):\n",
    "        \"\"\"Generate intelligent response using TinyLlama with food recommendations\"\"\"\n",
    "        \n",
    "        # Get food recommendations\n",
    "        recommendations = self.get_food_recommendations(user_input)\n",
    "        \n",
    "        # Get current user info for dietary checking\n",
    "        user_info = self.user_df[self.user_df['user_id'] == self.current_user].iloc[0]\n",
    "        \n",
    "        # Check if user is asking about a specific food that's incompatible\n",
    "        user_input_lower = user_input.lower()\n",
    "        dietary_conflicts = []\n",
    "        \n",
    "        if user_info['dietary_preference'] == 'vegan':\n",
    "            meat_words = ['chicken', 'beef', 'pork', 'fish', 'salmon', 'shrimp', 'meat', 'cheese', 'dairy']\n",
    "            dietary_conflicts = [word for word in meat_words if word in user_input_lower]\n",
    "        elif user_info['dietary_preference'] == 'vegetarian':\n",
    "            meat_words = ['chicken', 'beef', 'pork', 'fish', 'salmon', 'shrimp', 'meat']\n",
    "            dietary_conflicts = [word for word in meat_words if word in user_input_lower]\n",
    "        elif user_info['dietary_preference'] == 'pescatarian':\n",
    "            meat_words = ['chicken', 'beef', 'pork', 'meat']\n",
    "            dietary_conflicts = [word for word in meat_words if word in user_input_lower]\n",
    "        \n",
    "        # Create context with recommendations and user dietary info\n",
    "        rec_text = \"\\\\n\".join([\n",
    "            f\"‚Ä¢ {rec['food_name']} ({rec['category']}) - {rec['preference_score']:.2f} match score\"\n",
    "            for rec in recommendations\n",
    "        ])\n",
    "        \n",
    "        dietary_info = f\"User is {user_info['dietary_preference']} with {user_info['spice_tolerance']} spice tolerance.\"\n",
    "        if dietary_conflicts:\n",
    "            dietary_info += f\" IMPORTANT: User asked about {', '.join(dietary_conflicts)} which conflicts with their {user_info['dietary_preference']} diet.\"\n",
    "        \n",
    "        # Create conversation prompt\n",
    "        prompt = f\"\"\"<|system|>\n",
    "{self.system_prompt}\n",
    "\n",
    "{dietary_info}\n",
    "\n",
    "Current user's top food recommendations:\n",
    "{rec_text}\n",
    "<|user|>\n",
    "{user_input}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Tokenize and generate\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(self.llama_model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.llama_model.generate(\n",
    "                    **inputs,\n",
    "                    generation_config=self.generation_config,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only the assistant's response\n",
    "            if \"<|assistant|>\" in response:\n",
    "                response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "            \n",
    "            # Add explicit dietary conflict warning if needed\n",
    "            if dietary_conflicts:\n",
    "                conflict_warning = f\"‚ö†Ô∏è Note: As a {user_info['dietary_preference']}, I should mention that {', '.join(dietary_conflicts)} would not be suitable for your dietary preferences. \"\n",
    "                response = conflict_warning + response\n",
    "            \n",
    "            # Add conversation to history\n",
    "            self.conversation_history.append({\n",
    "                'user': user_input,\n",
    "                'assistant': response,\n",
    "                'recommendations': recommendations\n",
    "            })\n",
    "            \n",
    "            return response, recommendations\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Enhanced fallback with dietary awareness\n",
    "            if dietary_conflicts:\n",
    "                fallback_response = f\"‚ö†Ô∏è As a {user_info['dietary_preference']}, {', '.join(dietary_conflicts)} wouldn't be suitable for your diet. Instead, I'd recommend {recommendations[0]['food_name']} which matches your preferences with a {recommendations[0]['preference_score']:.2f} score!\"\n",
    "            else:\n",
    "                fallback_response = f\"I'd recommend trying {recommendations[0]['food_name']} - it matches your taste preferences with a {recommendations[0]['preference_score']:.2f} score!\"\n",
    "            return fallback_response, recommendations\n",
    "    \n",
    "    def clear_conversation(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"üóëÔ∏è Conversation history cleared\")\n",
    "\n",
    "# Initialize the chatbot\n",
    "print(\"ü§ñ Initializing Intelligent Food Chatbot...\")\n",
    "\n",
    "chatbot = FoodPreferenceChatbot(\n",
    "    llama_model=llama_model,\n",
    "    tokenizer=tokenizer,\n",
    "    two_tower_model=model,\n",
    "    generation_config=generation_config,\n",
    "    user_embeddings=user_embeddings,\n",
    "    food_embeddings=food_embeddings,\n",
    "    user_df=user_df,\n",
    "    food_df=food_df,\n",
    "    user_id_to_idx=user_id_to_idx,\n",
    "    food_id_to_idx=food_id_to_idx,\n",
    "    embedding_model=embedding_model\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Chatbot initialized successfully!\")\n",
    "print(\"üéØ Ready for intelligent food recommendations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9257e4",
   "metadata": {},
   "source": [
    "## üí¨ Interactive Chat Interface\n",
    "\n",
    "Experience the complete integration! Chat with the AI and get personalized food recommendations powered by both the trained Two-Tower model and TinyLlama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation with the intelligent chatbot\n",
    "print(\"üé≠ Chatbot Demo - Testing different user profiles\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with different users\n",
    "demo_queries = [\n",
    "    \"I'm looking for something spicy and flavorful for dinner tonight\",\n",
    "    \"What's a good healthy meal option?\", \n",
    "    \"I want comfort food that's not too heavy\",\n",
    "    \"Suggest something exotic and adventurous\",\n",
    "    \"I need a quick vegetarian meal\"\n",
    "]\n",
    "\n",
    "demo_users = [\"user_001\", \"user_002\", \"user_003\", \"user_004\", \"user_005\"]\n",
    "\n",
    "for i, (user_id, query) in enumerate(zip(demo_users, demo_queries)):\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(f\"üéØ Demo {i+1}: Testing {user_id}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Set user\n",
    "    success = chatbot.set_user(user_id)\n",
    "    if not success:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\\\nüí¨ User Query: '{query}'\")\n",
    "    print(\"\\\\nü§ñ AI Response:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get response\n",
    "    response, recommendations = chatbot.generate_response(query)\n",
    "    print(response)\n",
    "    \n",
    "    print(\"\\\\nüìä Top Recommendations with Scores:\")\n",
    "    for j, rec in enumerate(recommendations, 1):\n",
    "        print(f\"  {j}. {rec['food_name']} ({rec['category']})\")\n",
    "        print(f\"     Match Score: {rec['preference_score']:.3f} | Like Probability: {rec['like_probability']:.3f}\")\n",
    "        print(f\"     Ingredients: {rec['ingredients']}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"\\\\nüéâ Demo completed! The chatbot successfully:\")\n",
    "print(\"‚úÖ Loaded and used TinyLlama-1.1B for natural language generation\")\n",
    "print(\"‚úÖ Applied the trained Two-Tower model for personalized recommendations\") \n",
    "print(\"‚úÖ Combined both models for intelligent, context-aware responses\")\n",
    "print(\"‚úÖ Provided different recommendations based on individual user preferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bfab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and Setup Gradio Interface (RunPod Compatible)\n",
    "print(\"üéÆ Setting up Gradio Chat Interface...\")\n",
    "\n",
    "# Install Gradio\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gradio>=4.0.0\"], check=True)\n",
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Gradio installed successfully!\")\n",
    "\n",
    "# Global variables for the chat interface\n",
    "current_chat_user = \"user_001\"  # Default user\n",
    "chat_history = []\n",
    "\n",
    "def format_recommendations(recommendations):\n",
    "    \"\"\"Format recommendations for display\"\"\"\n",
    "    rec_text = \"üìä **Personalized Recommendations:**\\n\\n\"\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        score_emoji = \"üü¢\" if rec['preference_score'] > 0.7 else \"üü°\" if rec['preference_score'] > 0.5 else \"üü†\"\n",
    "        rec_text += f\"{score_emoji} **{i}. {rec['food_name']}** ({rec['category']})\\n\"\n",
    "        rec_text += f\"   ‚Ä¢ Match Score: {rec['preference_score']:.3f}\\n\"\n",
    "        rec_text += f\"   ‚Ä¢ Ingredients: {rec['ingredients']}\\n\"\n",
    "        rec_text += f\"   ‚Ä¢ Dietary: {'Vegetarian' if rec['is_vegetarian'] else 'Non-vegetarian'}\\n\"\n",
    "        rec_text += f\"   ‚Ä¢ Spice Level: {'Spicy' if rec['is_spicy'] else 'Mild'}\\n\\n\"\n",
    "    return rec_text\n",
    "\n",
    "def chat_with_ai(message, history, user_selection):\n",
    "    \"\"\"Main chat function for Gradio interface\"\"\"\n",
    "    global current_chat_user\n",
    "    \n",
    "    if not message.strip():\n",
    "        return history, \"\"\n",
    "    \n",
    "    # Update user if changed\n",
    "    if current_chat_user != user_selection:\n",
    "        current_chat_user = user_selection\n",
    "        chatbot.set_user(current_chat_user)\n",
    "        user_info = user_df[user_df['user_id'] == current_chat_user].iloc[0]\n",
    "        history.append([\n",
    "            f\"üîÑ Switched to {current_chat_user}\",\n",
    "            f\"üë§ **Profile:** {user_info['description']}\\nüìù **Diet:** {user_info['dietary_preference']}\\nüå∂Ô∏è **Spice:** {user_info['spice_tolerance']}\\nüåç **Cuisines:** {', '.join(user_info['favorite_cuisines'])}\"\n",
    "        ])\n",
    "    \n",
    "    try:\n",
    "        # Generate AI response\n",
    "        ai_response, recommendations = chatbot.generate_response(message)\n",
    "        \n",
    "        # Format the complete response\n",
    "        full_response = f\"ü§ñ **AI Response:**\\n{ai_response}\\n\\n\"\n",
    "        full_response += format_recommendations(recommendations)\n",
    "        \n",
    "        # Add to history\n",
    "        history.append([message, full_response])\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_response = f\"‚ùå **Error:** {str(e)}\\n\\nüîÑ Please try again or check the model loading.\"\n",
    "        history.append([message, error_response])\n",
    "    \n",
    "    return history, \"\"\n",
    "\n",
    "def clear_chat_history():\n",
    "    \"\"\"Clear the chat history\"\"\"\n",
    "    chatbot.clear_conversation()\n",
    "    return [], f\"üóëÔ∏è Chat history cleared! Current user: {current_chat_user}\"\n",
    "\n",
    "def get_user_info(user_selection):\n",
    "    \"\"\"Get user information for display\"\"\"\n",
    "    user_info = user_df[user_df['user_id'] == user_selection].iloc[0]\n",
    "    info_text = f\"\"\"\n",
    "## üë§ Current User Profile\n",
    "\n",
    "**User ID:** {user_selection}\n",
    "**Description:** {user_info['description']}\n",
    "**Age Group:** {user_info['age_group']}\n",
    "**Dietary Preference:** {user_info['dietary_preference']}\n",
    "**Spice Tolerance:** {user_info['spice_tolerance']}\n",
    "**Favorite Cuisines:** {', '.join(user_info['favorite_cuisines'])}\n",
    "\"\"\"\n",
    "    return info_text\n",
    "\n",
    "# Create user options for dropdown\n",
    "user_options = [(f\"{row['user_id']} - {row['description']}\", row['user_id']) for _, row in user_df.iterrows()]\n",
    "\n",
    "# Initialize chatbot with default user\n",
    "chatbot.set_user(current_chat_user)\n",
    "\n",
    "print(\"üöÄ Launching Gradio Interface...\")\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(\n",
    "    title=\"ü§ñ TinyLlama + Two-Tower Food Chatbot\",\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\"\"\"\n",
    "    .gradio-container {\n",
    "        max-width: 1200px !important;\n",
    "    }\n",
    "    .user-msg {\n",
    "        background-color: #e3f2fd;\n",
    "    }\n",
    "    .bot-msg {\n",
    "        background-color: #f3e5f5;\n",
    "    }\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # ü§ñ TinyLlama + Two-Tower Food Preference Chatbot\n",
    "    \n",
    "    **Complete Phase 4 Implementation:** Experience the full integration of TinyLlama-1.1B with a trained Two-Tower neural network for personalized food recommendations!\n",
    "    \n",
    "    ## üéØ Features:\n",
    "    - **üß† Trained Two-Tower Model:** Real neural network trained on food preference data\n",
    "    - **ü§ñ TinyLlama Integration:** 1.1B parameter language model for natural conversation\n",
    "    - **üë• Multiple User Profiles:** Switch between different user preferences\n",
    "    - **üìä Smart Recommendations:** AI-powered food suggestions with confidence scores\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            user_dropdown = gr.Dropdown(\n",
    "                choices=[choice[0] for choice in user_options],\n",
    "                value=user_options[0][0],\n",
    "                label=\"üë§ Select User Profile\",\n",
    "                info=\"Choose a user to get personalized recommendations\"\n",
    "            )\n",
    "            \n",
    "            user_info_display = gr.Markdown(\n",
    "                get_user_info(current_chat_user),\n",
    "                label=\"User Information\"\n",
    "            )\n",
    "            \n",
    "        with gr.Column(scale=2):\n",
    "            chatbot_interface = gr.Chatbot(\n",
    "                label=\"üí¨ Chat with AI Food Assistant\",\n",
    "                height=500,\n",
    "                show_label=True,\n",
    "                container=True,\n",
    "                bubble_full_width=False\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg_input = gr.Textbox(\n",
    "                    placeholder=\"Ask for food recommendations, cooking advice, or dietary suggestions...\",\n",
    "                    label=\"Your Message\",\n",
    "                    scale=4,\n",
    "                    container=False\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send üì§\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"Clear Chat üóëÔ∏è\", variant=\"secondary\")\n",
    "                \n",
    "    \n",
    "    # Event handlers\n",
    "    def update_user_info(user_selection):\n",
    "        # Extract user_id from selection\n",
    "        user_id = next(choice[1] for choice in user_options if choice[0] == user_selection)\n",
    "        return get_user_info(user_id)\n",
    "    \n",
    "    def process_message(message, history, user_selection):\n",
    "        # Extract user_id from selection\n",
    "        user_id = next(choice[1] for choice in user_options if choice[0] == user_selection)\n",
    "        return chat_with_ai(message, history, user_id)\n",
    "    \n",
    "    # Connect events\n",
    "    user_dropdown.change(\n",
    "        update_user_info,\n",
    "        inputs=[user_dropdown],\n",
    "        outputs=[user_info_display]\n",
    "    )\n",
    "    \n",
    "    send_btn.click(\n",
    "        process_message,\n",
    "        inputs=[msg_input, chatbot_interface, user_dropdown],\n",
    "        outputs=[chatbot_interface, msg_input]\n",
    "    )\n",
    "    \n",
    "    msg_input.submit(\n",
    "        process_message,\n",
    "        inputs=[msg_input, chatbot_interface, user_dropdown],\n",
    "        outputs=[chatbot_interface, msg_input]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        clear_chat_history,\n",
    "        outputs=[chatbot_interface, user_info_display]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "print(\"üåü Starting Gradio server...\")\n",
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",  # Allow external access\n",
    "    server_port=7860,       # Standard Gradio port\n",
    "    share=True,             # Create shareable link\n",
    "    show_error=True,        # Show errors in interface\n",
    "    quiet=False             # Show startup logs\n",
    ")\n",
    "\n",
    "print(\"\\\\nüéä Complete Implementation Ready!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Two-Tower neural network trained and evaluated\")\n",
    "print(\"‚úÖ TinyLlama-1.1B model loaded and integrated\")\n",
    "print(\"‚úÖ Intelligent chatbot combining both models\")\n",
    "print(\"‚úÖ Gradio interface launched successfully\")\n",
    "print(\"‚úÖ Personalized food recommendations powered by ML\")\n",
    "print(\"\\\\nüöÄ Access the chat interface using the URLs above!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
