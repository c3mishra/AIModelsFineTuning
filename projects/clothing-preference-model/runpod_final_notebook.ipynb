{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Tower Clothing Preference Model + TinyLLaMA Chat Interface\n",
    "\n",
    "This notebook implements an end-to-end pipeline for clothing preference prediction with LLM reasoning and a Gradio web interface.\n",
    "\n",
    "Environment: Optimized for RunPod GPU instances (â‰¤16GB VRAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEPENDENCY INSTALLATION FOR RUNPOD (Including TinyLLaMA)\n",
    "# =============================================================================\n",
    "import subprocess, sys\n",
    "\n",
    "def pip_install(pkg):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--upgrade', '--no-cache-dir'])\n",
    "        print(f'âœ… Installed: {pkg}')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'âŒ Failed: {pkg} -> {e}')\n",
    "\n",
    "# IMPORTANT: Pin Pillow to a compatible version to avoid EXIF import errors\n",
    "packages = [\n",
    "    'torch', 'torchvision', 'numpy', 'pandas', 'matplotlib', 'seaborn',\n",
    "    'pillow>=10.2.0,<11',\n",
    "    'transformers>=4.30.0', 'accelerate>=0.20.0', 'tokenizers>=0.13.0', 'sentencepiece',\n",
    "    'typing_extensions>=4.12.2', 'gradio>=4.0.0', 'fastapi>=0.100.0', 'uvicorn[standard]', 'websockets', 'httpx'\n",
    "]\n",
    "print('ðŸš€ Installing required packages for RunPod (including TinyLLaMA & Gradio)...')\n",
    "for p in packages:\n",
    "    name = p.split('>=')[0].split('==')[0].replace('-', '_')\n",
    "    # Always force-upgrade typing_extensions to ensure TypeIs is available\n",
    "    if name == 'typing_extensions':\n",
    "        print('ðŸ“¦ Forcing upgrade of typing_extensions ...')\n",
    "        pip_install(p)\n",
    "        continue\n",
    "    try:\n",
    "        __import__(name)\n",
    "        print(f'âœ… {name} already installed')\n",
    "    except Exception:\n",
    "        print(f'ðŸ“¦ Installing {p} ...')\n",
    "        pip_install(p)\n",
    "print('âœ… Dependencies ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN IMPORTS AND DEVICE SETUP\n",
    "# =============================================================================\n",
    "import torch, torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision, torchvision.transforms as transforms\n",
    "import numpy as np, random, warnings\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "from PIL import Image\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print('ðŸš€ Using CUDA')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        return torch.device('cuda')\n",
    "    print('âŒ CUDA not available - CPU')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = setup_device()\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMAGE/CHANNEL CONFIGURATION\n",
    "# =============================================================================\n",
    "IMAGE_CHANNELS = 3  # set to 1 to remain pure grayscale; 3 replicates grayscale to RGB\n",
    "IMG_SIZE = 28       # keep 28x28 for Fashion-MNIST; can change if using a larger backbone later\n",
    "\n",
    "def _norm_tuples(ch):\n",
    "    mean = tuple([0.5]*ch)\n",
    "    std = tuple([0.5]*ch)\n",
    "    return mean, std\n",
    "\n",
    "NORM_MEAN, NORM_STD = _norm_tuples(IMAGE_CHANNELS)\n",
    "print(f\"ðŸ§© Image config -> channels: {IMAGE_CHANNELS}, size: {IMG_SIZE}x{IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM DATA ROOT CONFIGURATION\n",
    "# =============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Default to a folder next to this notebook: projects/clothing-preference-model/data/custom\n",
    "# Since the working directory is typically the notebook's folder, use a relative path\n",
    "CUSTOM_DATA_ROOT = os.environ.get('CUSTOM_DATA_ROOT', str(Path('data') / 'custom'))\n",
    "print(f\"ðŸ“‚ CUSTOM_DATA_ROOT = {CUSTOM_DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "FASHION_CLASSES = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "print('ðŸ“¥ Loading Fashion-MNIST...')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=IMAGE_CHANNELS) if IMAGE_CHANNELS==3 else transforms.Lambda(lambda x: x),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(NORM_MEAN, NORM_STD)\n",
    "])\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "print(f'âœ… Train: {len(train_dataset)} | Test: {len(test_dataset)} | Channels: {IMAGE_CHANNELS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Custom User Image Data (Optional)\n",
    "\n",
    "> Folder layout:\n",
    "```\n",
    "projects/clothing-preference-model/data/custom/\n",
    "  user_0/\n",
    "    likes/        # images the user likes\n",
    "    dislikes/     # images the user dislikes\n",
    "    # optional: likes/Dress/*.jpg (class subfolders)\n",
    "```\n",
    "- User id is inferred from folder name: user_0 -> 0\n",
    "- If class subfolders are used (matching entries in FASHION_CLASSES), we train the classifier too; else class loss is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM USER DATASET HELPERS\n",
    "# =============================================================================\n",
    "import os, glob\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def _class_from_path(path: str):\n",
    "    # Try to detect a class name from immediate subfolder under likes/dislikes that matches FASHION_CLASSES\n",
    "    parts = os.path.normpath(path).split(os.sep)\n",
    "    for p in reversed(parts):\n",
    "        if p in FASHION_CLASSES:\n",
    "            return FASHION_CLASSES.index(p)\n",
    "    return None\n",
    "\n",
    "def _pil_to_tensor(pil_img: PILImage.Image):\n",
    "    # Apply same preprocessing as UI/dataset\n",
    "    if IMAGE_CHANNELS == 3 and pil_img.mode != 'RGB':\n",
    "        pil_img = pil_img.convert('RGB')\n",
    "    if IMAGE_CHANNELS == 1 and pil_img.mode != 'L':\n",
    "        pil_img = pil_img.convert('L')\n",
    "    pil_img = pil_img.resize((IMG_SIZE, IMG_SIZE))\n",
    "    return _def_transform(pil_img) if '_def_transform' in globals() else transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=IMAGE_CHANNELS) if IMAGE_CHANNELS==3 else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(NORM_MEAN, NORM_STD)\n",
    "    ])(pil_img)\n",
    "\n",
    "def load_custom_user_samples(root_dir=None) -> list:\n",
    "    \"\"\"Return a list of dicts: {'user_id','image','preference','label'(optional)}\"\"\"\n",
    "    if root_dir is None:\n",
    "        root_dir = CUSTOM_DATA_ROOT if 'CUSTOM_DATA_ROOT' in globals() else 'projects/clothing-preference-model/data/custom'\n",
    "    samples = []\n",
    "    if not os.path.isdir(root_dir):\n",
    "        print(f\"(custom) root not found: {root_dir}\")\n",
    "        return samples\n",
    "    users = sorted([d for d in os.listdir(root_dir) if d.startswith('user_') and os.path.isdir(os.path.join(root_dir, d))])\n",
    "    for ud in users:\n",
    "        try: uid = int(ud.split('_')[-1])\n",
    "        except: continue\n",
    "        like_dir = os.path.join(root_dir, ud, 'likes')\n",
    "        dislike_dir = os.path.join(root_dir, ud, 'dislikes')\n",
    "        for lbl, ddir in [(1, like_dir), (0, dislike_dir)]:\n",
    "            if not os.path.isdir(ddir):\n",
    "                continue\n",
    "            for fp in glob.glob(os.path.join(ddir, '**', '*.*'), recursive=True):\n",
    "                if not fp.lower().endswith(('.jpg','.jpeg','.png','.bmp','.webp')):\n",
    "                    continue\n",
    "                try:\n",
    "                    pil = PILImage.open(fp).convert('RGB' if IMAGE_CHANNELS==3 else 'L')\n",
    "                    tensor = _pil_to_tensor(pil)\n",
    "                    cls_idx = _class_from_path(fp)\n",
    "                    entry = {'user_id': uid, 'image': tensor, 'preference': lbl}\n",
    "                    if cls_idx is not None:\n",
    "                        entry['label'] = int(cls_idx)\n",
    "                    samples.append(entry)\n",
    "                except Exception as e:\n",
    "                    print(f'âš ï¸ Skip {fp}: {e}')\n",
    "    print(f'ðŸ“¦ Loaded custom samples: {len(samples)} from {root_dir}')\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘¤ User Profiles (Likes/Dislikes/Style)\n",
    "\n",
    "> We define rich user profiles with names, preferred styles, and per-class preferences. These profiles drive training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USER PROFILES CONFIG\n",
    "# =============================================================================\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class UserProfile:\n",
    "    user_id: int\n",
    "    name: str\n",
    "    style: str  # e.g., 'casual', 'formal', 'sporty', 'minimal'\n",
    "    likes: List[int]  # class indices\n",
    "    dislikes: List[int]  # class indices\n",
    "    neutral: List[int]  # class indices\n",
    "\n",
    "USER_PROFILES: Dict[int, UserProfile] = {\n",
    "    0: UserProfile(\n",
    "        user_id=0,\n",
    "        name='Joan Doe',\n",
    "        style='casual, comfortable',\n",
    "        likes=[0, 2, 3, 6],  # T-shirt/top, Pullover, Dress, Shirt\n",
    "        dislikes=[4, 8],     # Coat, Bag (prefers wearable over accessories/formal layers)\n",
    "        neutral=[1, 5, 7, 9],  # Trouser, Sandal, Sneaker, Ankle boot\n",
    "    ),\n",
    "    1: UserProfile(\n",
    "        user_id=1,\n",
    "        name='Alex Kim',\n",
    "        style='sporty athleisure',\n",
    "        likes=[1, 5, 7, 9],  # Trouser, Sandal, Sneaker, Ankle boot\n",
    "        dislikes=[3, 8],\n",
    "        neutral=[0, 2, 4, 6],\n",
    "    ),\n",
    "    2: UserProfile(\n",
    "        user_id=2,\n",
    "        name='Riya Singh',\n",
    "        style='smart casual',\n",
    "        likes=[3, 4, 6],\n",
    "        dislikes=[5],\n",
    "        neutral=[0, 1, 2, 7, 8, 9],\n",
    "    ),\n",
    "    3: UserProfile(\n",
    "        user_id=3,\n",
    "        name='Diego LÃ³pez',\n",
    "        style='minimal formal',\n",
    "        likes=[1, 4, 6],\n",
    "        dislikes=[5, 9],\n",
    "        neutral=[0, 2, 3, 7, 8],\n",
    "    ),\n",
    "}\n",
    "\n",
    "def build_preference_lookup(user_profiles: Dict[int, UserProfile]) -> Dict[int, Dict[int, float]]:\n",
    "    \"\"\"Map user_id -> class_id -> base preference weight (0..1).\"\"\"\n",
    "    lookup: Dict[int, Dict[int, float]] = {}\n",
    "    for uid, prof in user_profiles.items():\n",
    "        m = {c: 0.5 for c in range(len(FASHION_CLASSES))}\n",
    "        for c in prof.likes: m[c] = 0.9\n",
    "        for c in prof.dislikes: m[c] = 0.1\n",
    "        for c in prof.neutral: m[c] = min(m.get(c, 0.5), 0.5)\n",
    "        lookup[uid] = m\n",
    "    return lookup\n",
    "\n",
    "PROFILE_PREFS = build_preference_lookup(USER_PROFILES)\n",
    "print('ðŸ‘¤ Loaded user profiles:', {uid: p.name for uid, p in USER_PROFILES.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL\n",
    "# =============================================================================\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_users=1000, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        in_ch = IMAGE_CHANNELS if 'IMAGE_CHANNELS' in globals() else 1\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim), nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        self.item_cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_ch,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((4,4))\n",
    "        )\n",
    "        self._feat_dim = 128*4*4\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(self._feat_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim), nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self._feat_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 10)\n",
    "        )\n",
    "\n",
    "    def _item_features(self, item_images):\n",
    "        return self.item_cnn(item_images).view(item_images.size(0), -1)\n",
    "\n",
    "    def forward(self, user_ids, item_images):\n",
    "        u = self.user_tower(self.user_embedding(user_ids))\n",
    "        feat = self._item_features(item_images)\n",
    "        v = self.item_tower(feat)\n",
    "        return u, v\n",
    "\n",
    "    def predict_preference(self, user_ids, item_images):\n",
    "        u, v = self.forward(user_ids, item_images)\n",
    "        u, v = F.normalize(u, p=2, dim=1), F.normalize(v, p=2, dim=1)\n",
    "        return torch.sigmoid(torch.sum(u*v, dim=1))\n",
    "\n",
    "    def classify(self, item_images):\n",
    "        feat = self._item_features(item_images)\n",
    "        return self.classifier(feat)\n",
    "\n",
    "# Determine num_users from profiles if available\n",
    "_num_users = 1000\n",
    "try:\n",
    "    if 'USER_PROFILES' in globals() and len(USER_PROFILES) > 0:\n",
    "        _num_users = max(1000, max(USER_PROFILES.keys())+1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model = TwoTowerModel(num_users=_num_users).to(device)\n",
    "print(f'âœ… Model params: {sum(p.numel() for p in model.parameters()):,} | Users capacity: {_num_users} | InCh: {IMAGE_CHANNELS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING SETUP\n",
    "# =============================================================================\n",
    "def create_preference_data(dataset, num_users=1000, num_samples=2000):\n",
    "    using_profiles = 'PROFILE_PREFS' in globals() and isinstance(PROFILE_PREFS, dict) and len(PROFILE_PREFS) > 0\n",
    "    prefs = []\n",
    "    for _ in range(num_samples):\n",
    "        uid = random.choice(list(PROFILE_PREFS.keys())) if using_profiles else random.randint(0, num_users-1)\n",
    "        idx = random.randint(0, len(dataset)-1)\n",
    "        img, label = dataset[idx]\n",
    "        if using_profiles:\n",
    "            base = PROFILE_PREFS.get(uid, {}).get(int(label), 0.5)\n",
    "            jitter = random.uniform(-0.1, 0.1)\n",
    "            prob = min(max(base + jitter, 0.0), 1.0)\n",
    "            pref = 1 if random.random() < prob else 0\n",
    "        else:\n",
    "            user_prefs = {uid % 3:[0,1,2], (uid+1)%3:[3,4,6], (uid+2)%3:[5,7,9]}.get(uid%3, list(range(10)))\n",
    "            pref = 1 if (label in user_prefs and random.random()>0.2) or (label not in user_prefs and random.random()>0.7) else 0\n",
    "        prefs.append({'user_id': uid, 'image': img, 'preference': pref, 'label': int(label)})\n",
    "    return prefs\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, prefs): self.prefs = prefs\n",
    "    def __len__(self): return len(self.prefs)\n",
    "    def __getitem__(self, i):\n",
    "        it = self.prefs[i]\n",
    "        uid = torch.tensor(it['user_id'], dtype=torch.long)\n",
    "        img = it['image']\n",
    "        pref = torch.tensor(it['preference'], dtype=torch.float)\n",
    "        label = torch.tensor(it['label'], dtype=torch.long) if 'label' in it else torch.tensor(-1, dtype=torch.long)\n",
    "        return (uid, img, pref, label)\n",
    "\n",
    "print('ðŸŽ¯ Preparing training data...')\n",
    "# Try custom samples first\n",
    "custom_samples = load_custom_user_samples()\n",
    "if len(custom_samples) > 0:\n",
    "    preference_data = custom_samples\n",
    "    print('âœ… Using custom user data for training')\n",
    "else:\n",
    "    print('â„¹ï¸ No custom data found; generating profile-based synthetic samples')\n",
    "    _num_users_create = _num_users if '_num_users' in globals() else 1000\n",
    "    preference_data = create_preference_data(train_dataset, num_users=_num_users_create, num_samples=3000)\n",
    "\n",
    "train_loader = DataLoader(PreferenceDataset(preference_data), batch_size=32, shuffle=True)\n",
    "\n",
    "bce = nn.BCELoss()\n",
    "ce = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(f\"âœ… Training setup complete | Samples: {len(preference_data)} | Using profiles: {'yes' if 'PROFILE_PREFS' in globals() else 'no'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN LOOP\n",
    "# =============================================================================\n",
    "def train_model(model, loader, epochs=3, alpha_cls=0.3):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        ep_loss, correct_pref, total_pref = 0.0, 0, 0\n",
    "        correct_cls, total_cls = 0, 0\n",
    "        for b,(uids, imgs, prefs, labels) in enumerate(loader):\n",
    "            uids, imgs, prefs, labels = uids.to(device), imgs.to(device), prefs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds_pref = model.predict_preference(uids, imgs)\n",
    "            logits = model.classify(imgs)\n",
    "            loss_pref = bce(preds_pref, prefs)\n",
    "            # Class loss if labels are provided (>=0)\n",
    "            has_cls = (labels >= 0)\n",
    "            if has_cls.any():\n",
    "                loss_cls = ce(logits[has_cls], labels[has_cls])\n",
    "                loss = loss_pref + alpha_cls * loss_cls\n",
    "            else:\n",
    "                loss_cls = torch.tensor(0.0, device=device)\n",
    "                loss = loss_pref\n",
    "            loss.backward(); optimizer.step()\n",
    "            ep_loss += loss.item()\n",
    "            pred_bin = (preds_pref>0.5).float(); total_pref += prefs.size(0); correct_pref += (pred_bin==prefs).sum().item()\n",
    "            if has_cls.any():\n",
    "                pred_cls = logits.argmax(dim=1)\n",
    "                # Count only where class labels exist\n",
    "                correct_cls += (pred_cls[has_cls]==labels[has_cls]).sum().item()\n",
    "                total_cls += has_cls.sum().item()\n",
    "            if b % 20 == 0: print(f'Ep {ep+1} B{b} Loss {loss.item():.4f} | Pref {loss_pref.item():.4f} | Cls {loss_cls.item():.4f}')\n",
    "        msg_cls = f\" | ClsAcc {(100.*correct_cls/max(1,total_cls)):.2f}%\" if total_cls>0 else \"\"\n",
    "        print(f'Epoch {ep+1} - Loss {ep_loss/len(loader):.4f} | PrefAcc {(100.*correct_pref/total_pref):.2f}%{msg_cls}')\n",
    "        if device.type=='cuda': torch.cuda.empty_cache()\n",
    "\n",
    "train_model(model, train_loader)\n",
    "print('âœ… Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– LLM Integration (TinyLLaMA default; gpt-oss-20b optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURABLE LLM (TinyLLaMA default; gpt-oss-20b optional)\n",
    "# =============================================================================\n",
    "LLM_PROVIDER = globals().get('LLM_PROVIDER', 'tinyllama')  # 'tinyllama' | 'gpt-oss-20b'\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    _llama_ok = False\n",
    "    _llm_name = None\n",
    "    print(f'ðŸ¤– Initializing LLM provider: {LLM_PROVIDER}')\n",
    "    if LLM_PROVIDER.lower() in ['gpt-oss-20b', 'gpt_oss_20b', 'oss20b']:\n",
    "        _llm_name = 'openai/gpt-oss-20b'\n",
    "    else:\n",
    "        _llm_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "    _tok = AutoTokenizer.from_pretrained(_llm_name)\n",
    "    _llm = AutoModelForCausalLM.from_pretrained(\n",
    "        _llm_name,\n",
    "        torch_dtype=(torch.float16 if device.type=='cuda' else torch.float32),\n",
    "        device_map=('auto' if device.type=='cuda' else None)\n",
    "    )\n",
    "    if _tok.pad_token is None:\n",
    "        _tok.pad_token = _tok.eos_token\n",
    "    _llama_ok = True\n",
    "    print(f'âœ… LLM ready: {_llm_name}')\n",
    "\n",
    "    def _format_prompt(system_msg: str, user_msg: str):\n",
    "        # Support both chat-tuned TinyLLaMA and generic causal models\n",
    "        if 'TinyLlama' in _llm_name:\n",
    "            return f\"<|system|>\\n{system_msg}\\n\\n<|user|>\\n{user_msg}\\n\\n<|assistant|>\"\n",
    "        else:\n",
    "            # For gpt-oss-20b (causal), use a simple instruction format\n",
    "            return f\"System: {system_msg}\\nUser: {user_msg}\\nAssistant:\"\n",
    "\n",
    "    def generate_explanation(score, item_class, user_id):\n",
    "        user_context = ''\n",
    "        try:\n",
    "            if 'USER_PROFILES' in globals() and int(user_id) in USER_PROFILES:\n",
    "                p = USER_PROFILES[int(user_id)]\n",
    "                user_context = f\"User name: {p.name}. Style: {p.style}. Likes: {p.likes}. Dislikes: {p.dislikes}. \"\n",
    "        except Exception:\n",
    "            pass\n",
    "        sys_prompt = 'You are a fashion AI assistant explaining clothing preferences.'\n",
    "        user_prompt = (\n",
    "            f\"{user_context}A user (ID: {user_id}) has a preference score of {score:.2f} for a {item_class}. \"\n",
    "            \"Explain this preference in 1-2 sentences, considering that 1.0 is highest preference and 0.0 is lowest.\"\n",
    "        )\n",
    "        prompt = _format_prompt(sys_prompt, user_prompt)\n",
    "        inputs = _tok(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
    "        if device.type=='cuda':\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            out = _llm.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=(120 if 'gpt-oss-20b' in _llm_name else 50),\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=_tok.eos_token_id\n",
    "            )\n",
    "        text = _tok.decode(out[0], skip_special_tokens=True)\n",
    "        if 'TinyLlama' in _llm_name and '<|assistant|>' in text:\n",
    "            return text.split('<|assistant|>')[-1].strip()\n",
    "        # For generic models, strip trailing artifacts\n",
    "        return text.split('Assistant:')[-1].strip() if 'Assistant:' in text else text.strip()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'âš ï¸ LLM not available: {e}')\n",
    "    def generate_explanation(score, item_class, user_id):\n",
    "        return ('Very strong' if score>0.7 else 'Moderate' if score>0.4 else 'Low') + f' interest in {item_class} (score {score:.2f}).'\n",
    "    _llama_ok = False\n",
    "\n",
    "print('ðŸ§  Explanation system:', _llm_name if '_llm_name' in globals() else 'Fallback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PILLOW SANITY CHECK (EXIF & TIFF support)\n",
    "# =============================================================================\n",
    "try:\n",
    "    from PIL import Image, TiffImagePlugin\n",
    "    from PIL import __version__ as PIL_VERSION\n",
    "    print(f\"ðŸ–¼ï¸ Pillow version: {PIL_VERSION}\")\n",
    "    # Create a simple grayscale image and call getexif safely\n",
    "    im = Image.new('L', (8, 8), color=128)\n",
    "    exif = getattr(im, 'getexif', lambda: {})()\n",
    "    print(\"âœ… Pillow EXIF access OK\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pillow sanity check failed: {e}\")\n",
    "    print(\"ðŸ”§ Reinstalling Pillow to a compatible version...\")\n",
    "    import subprocess, sys\n",
    "    subprocess.call([sys.executable, '-m', 'pip', 'install', 'pillow>=10.2.0,<11', '--upgrade', '--no-cache-dir'])\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        print(\"âœ… Pillow reinstalled successfully\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Pillow import still failing: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Inference & Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TESTING\n",
    "# =============================================================================\n",
    "def _user_display(uid: int):\n",
    "    if 'USER_PROFILES' in globals() and uid in USER_PROFILES:\n",
    "        p = USER_PROFILES[uid]\n",
    "        return f\"{p.name} (style: {p.style})\"\n",
    "    return f\"User {uid}\"\n",
    "\n",
    "def _fuse_with_profile(uid: int, cls_idx: int, score: float):\n",
    "    base = PROFILE_PREFS.get(uid, {}).get(int(cls_idx), 0.5) if 'PROFILE_PREFS' in globals() else 0.5\n",
    "    w = 0.6 if base < 0.3 else 0.5 if 0.3 <= base <= 0.7 else 0.6\n",
    "    return (w*base + (1-w)*score)\n",
    "\n",
    "def manual_predict(user_id=0, sample_idx=0, pil_image=None):\n",
    "    model.eval()\n",
    "    if pil_image is not None:\n",
    "        x = _preprocess_image_for_model(pil_image)\n",
    "        label = None\n",
    "    else:\n",
    "        img, label = test_dataset[sample_idx % len(test_dataset)]\n",
    "        x = img.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        u = torch.tensor([int(user_id)]).to(device)\n",
    "        score_raw = model.predict_preference(u, x).item()\n",
    "        cls_logits = model.classify(x)\n",
    "        pred_cls = int(cls_logits.argmax(dim=1).item())\n",
    "    fused = _fuse_with_profile(int(user_id), pred_cls, score_raw)\n",
    "    cls_name = FASHION_CLASSES[pred_cls]\n",
    "    expl = generate_explanation(fused, cls_name, int(user_id))\n",
    "    prior = PROFILE_PREFS.get(int(user_id), {}).get(pred_cls, 0.5) if 'PROFILE_PREFS' in globals() else 0.5\n",
    "    verdict = 'dislike' if prior < 0.3 and fused < 0.5 else ('like' if fused > 0.6 else 'uncertain')\n",
    "    res = {\n",
    "        'user': _user_display(int(user_id)),\n",
    "        'predicted_class': cls_name,\n",
    "        'model_score_raw': round(score_raw,3),\n",
    "        'profile_prior': round(prior,3),\n",
    "        'fused_score': round(fused,3),\n",
    "        'verdict': verdict,\n",
    "        'explanation': expl\n",
    "    }\n",
    "    print(res); return res\n",
    "\n",
    "def test_multiple_users(sample_idx=0, n=3):\n",
    "    return [manual_predict(u, sample_idx) for u in range(n)]\n",
    "\n",
    "def test_multiple_items(user_id=0, n=3):\n",
    "    return [manual_predict(user_id, i) for i in range(n)]\n",
    "\n",
    "print('âœ… Testing helpers ready')\n",
    "manual_predict(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Gradio Web Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GRADIO (with port handling + TinyLLaMA chat)\n",
    "# =============================================================================\n",
    "import socket\n",
    "\n",
    "def find_free_port(start=7860, attempts=20):\n",
    "    for p in range(start, start+attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('', p)); return p\n",
    "        except OSError:\n",
    "            continue\n",
    "    return 0\n",
    "\n",
    "port = find_free_port()\n",
    "print(f'ðŸ”Œ Port selected: {port}')\n",
    "\n",
    "# Common image preprocessing used by both prediction and chat\n",
    "_def_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=IMAGE_CHANNELS) if IMAGE_CHANNELS==3 else transforms.Lambda(lambda x: x),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(NORM_MEAN, NORM_STD)\n",
    "])\n",
    "\n",
    "def _preprocess_image_for_model(image):\n",
    "    from PIL import Image as PILImage\n",
    "    if image is None:\n",
    "        raise ValueError('No image provided')\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = PILImage.fromarray(image)\n",
    "    if IMAGE_CHANNELS == 3 and image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    if IMAGE_CHANNELS == 1 and image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "    image = image.resize((IMG_SIZE, IMG_SIZE))\n",
    "    x = _def_transform(image).unsqueeze(0).to(device)\n",
    "    return x\n",
    "\n",
    "try:\n",
    "    # Sanity check typing_extensions.TypeIs presence before importing gradio\n",
    "    try:\n",
    "        from typing_extensions import TypeIs  # type: ignore\n",
    "        print('ðŸ”Ž typing_extensions.TypeIs OK')\n",
    "    except Exception as te:\n",
    "        print(f'âš ï¸ typing_extensions.TypeIs missing: {te}. Attempting quick reinstall...')\n",
    "        import subprocess, sys\n",
    "        subprocess.call([sys.executable, '-m', 'pip', 'install', 'typing_extensions>=4.12.2', '--upgrade', '--no-cache-dir'])\n",
    "        from typing_extensions import TypeIs  # retry\n",
    "        print('âœ… typing_extensions.TypeIs available after upgrade')\n",
    "\n",
    "    import gradio as gr\n",
    "    from PIL import Image as PILImage\n",
    "\n",
    "    def _user_display(uid: int):\n",
    "        if 'USER_PROFILES' in globals() and uid in USER_PROFILES:\n",
    "            p = USER_PROFILES[uid]\n",
    "            return f\"{p.name} (style: {p.style})\"\n",
    "        return f\"User {uid}\"\n",
    "\n",
    "    def _fuse_with_profile(uid: int, cls_idx: int, score: float):\n",
    "        base = PROFILE_PREFS.get(uid, {}).get(int(cls_idx), 0.5) if 'PROFILE_PREFS' in globals() else 0.5\n",
    "        w = 0.6 if base < 0.3 else 0.5 if 0.3 <= base <= 0.7 else 0.6\n",
    "        return (w*base + (1-w)*score)\n",
    "\n",
    "    def predict_ui(user_id, image):\n",
    "        if image is None:\n",
    "            return {'Error': 'Upload an image or use sample'}\n",
    "        try:\n",
    "            x = _preprocess_image_for_model(image)\n",
    "            u = torch.tensor([int(user_id)]).to(device)\n",
    "            with torch.no_grad():\n",
    "                score_raw = model.predict_preference(u, x).item()\n",
    "                logits = model.classify(x)\n",
    "                pred_cls = int(logits.argmax(dim=1).item())\n",
    "            fused = _fuse_with_profile(int(user_id), pred_cls, score_raw)\n",
    "            user_info = _user_display(int(user_id))\n",
    "            cls_name = FASHION_CLASSES[pred_cls]\n",
    "            prior = PROFILE_PREFS.get(int(user_id), {}).get(pred_cls, 0.5) if 'PROFILE_PREFS' in globals() else 0.5\n",
    "            verdict = 'dislike' if prior < 0.3 and fused < 0.5 else ('like' if fused > 0.6 else 'uncertain')\n",
    "            return {\n",
    "                'User': user_info,\n",
    "                'Predicted Class': cls_name,\n",
    "                'Model Score (raw)': f'{score_raw:.3f}',\n",
    "                'Profile Prior': f'{prior:.3f}',\n",
    "                'Fused Score': f'{fused:.3f}',\n",
    "                'Verdict': verdict,\n",
    "                'Explanation': generate_explanation(fused, cls_name, int(user_id))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'Error': str(e)}\n",
    "\n",
    "    def sample_img():\n",
    "        i = random.randint(0, len(test_dataset)-1)\n",
    "        img, _ = test_dataset[i]\n",
    "        pil = transforms.ToPILImage()(img)\n",
    "        if IMAGE_CHANNELS == 3 and pil.mode != 'RGB':\n",
    "            pil = pil.convert('RGB')\n",
    "        if IMAGE_CHANNELS == 1 and pil.mode != 'L':\n",
    "            pil = pil.convert('L')\n",
    "        return pil\n",
    "\n",
    "    def chat_about_outfit(user_id, image, message):\n",
    "        if image is None:\n",
    "            return 'Please upload an image first.'\n",
    "        try:\n",
    "            x = _preprocess_image_for_model(image)\n",
    "            u = torch.tensor([int(user_id)]).to(device)\n",
    "            with torch.no_grad():\n",
    "                score_raw = model.predict_preference(u, x).item()\n",
    "                logits = model.classify(x)\n",
    "                pred_cls = int(logits.argmax(dim=1).item())\n",
    "            fused = _fuse_with_profile(int(user_id), pred_cls, score_raw)\n",
    "            cls_name = FASHION_CLASSES[pred_cls]\n",
    "            prior = PROFILE_PREFS.get(int(user_id), {}).get(pred_cls, 0.5) if 'PROFILE_PREFS' in globals() else 0.5\n",
    "            profile_line = ''\n",
    "            if 'USER_PROFILES' in globals() and int(user_id) in USER_PROFILES:\n",
    "                p = USER_PROFILES[int(user_id)]\n",
    "                profile_line = f\"User: {p.name} (style: {p.style})\\nLikes: {p.likes} Dislikes: {p.dislikes}\\n\"\n",
    "            context = (\n",
    "                f\"{profile_line}Predicted item class: {cls_name}\\n\"\n",
    "                f\"Model score (raw): {score_raw:.3f} | Profile prior: {prior:.3f} | Fused: {fused:.3f}\\n\"\n",
    "                f\"Interpretation: {'high' if fused>0.7 else 'moderate' if fused>0.4 else 'low'} interest\\n\"\n",
    "            )\n",
    "            if '_llm' in globals() and '_tok' in globals() and globals().get('_llama_ok', False):\n",
    "                sys_prompt = (\n",
    "                    \"You are a helpful fashion AI assistant. Use context and be decisive.\\n\"\n",
    "                    \"If profile strongly dislikes the predicted class and fused<0.5, clearly recommend against it and suggest alternatives.\"\n",
    "                )\n",
    "                full_prompt = _format_prompt(sys_prompt, f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{message}\")\n",
    "                inputs = _tok(full_prompt, return_tensors='pt', truncation=True, max_length=768)\n",
    "                if device.type == 'cuda':\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    out = _llm.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=(180 if 'gpt-oss-20b' in _llm_name else 120),\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=_tok.eos_token_id\n",
    "                    )\n",
    "                text = _tok.decode(out[0], skip_special_tokens=True)\n",
    "                if 'TinyLlama' in _llm_name and '<|assistant|>' in text:\n",
    "                    reply = text.split('<|assistant|>')[-1].strip()\n",
    "                else:\n",
    "                    reply = text.split('Assistant:')[-1].strip() if 'Assistant:' in text else text.strip()\n",
    "            else:\n",
    "                expl = generate_explanation(fused, cls_name, int(user_id))\n",
    "                if prior < 0.3 and fused < 0.5:\n",
    "                    reply = f\"Noâ€”this looks too formal for the user. Try casual options like tees, pullovers, or sneakers. (Fused {fused:.3f})\\n{expl}\"\n",
    "                else:\n",
    "                    reply = (\n",
    "                        f\"Based on fused score {fused:.3f}, the user shows \"\n",
    "                        f\"{'high' if fused>0.7 else 'moderate' if fused>0.4 else 'low'} interest. \"\n",
    "                        f\"Explanation: {expl}\"\n",
    "                    )\n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            return f'Error: {e}'\n",
    "\n",
    "    with gr.Blocks(title='Two-Tower Clothing Preference Model') as demo:\n",
    "        gr.Markdown('# ðŸ‘— Two-Tower Clothing Preference Model')\n",
    "        with gr.Tabs():\n",
    "            with gr.Tab('Predict'):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        uid = gr.Number(label='User ID', value=0, minimum=0, maximum=999)\n",
    "                        img = gr.Image(label='Upload Clothing Image', type='pil', height=300)\n",
    "                        btn = gr.Button('ðŸ”® Predict Preference')\n",
    "                        sample = gr.Button('ðŸŽ² Random Sample')\n",
    "                    with gr.Column():\n",
    "                        out = gr.JSON(label='Results')\n",
    "                btn.click(predict_ui, inputs=[uid, img], outputs=out)\n",
    "                sample.click(sample_img, outputs=img)\n",
    "            with gr.Tab('Chat'):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        chat_uid = gr.Number(label='User ID', value=0, minimum=0, maximum=999)\n",
    "                        chat_img = gr.Image(label='Upload Outfit Image', type='pil', height=300)\n",
    "                        chat_q = gr.Textbox(label='Ask LLM', placeholder='Would user like this clothing outfit?', lines=2)\n",
    "                        chat_btn = gr.Button('ðŸ’¬ Ask')\n",
    "                    with gr.Column():\n",
    "                        chat_out = gr.Textbox(label='LLM Response')\n",
    "                chat_btn.click(chat_about_outfit, inputs=[chat_uid, chat_img, chat_q], outputs=chat_out)\n",
    "\n",
    "    print('ðŸŽ¨ Gradio ready; launching...')\n",
    "    demo.launch(server_name='0.0.0.0', server_port=(port if port>0 else None), share=True, show_error=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'âš ï¸ Gradio not available or failed: {e}')\n",
    "    print('ðŸ’¡ If this is a typing_extensions issue, rerun the first dependency cell to upgrade to >=4.12.2 and restart the kernel.')\n",
    "    print('ðŸ’¡ Use manual_predict(user_id, sample_idx) and chat_about_outfit(user_id, image, message) for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Summary\n",
    "\n",
    "- Standard Jupyter JSON format (RunPod compatible)\n",
    "- Two-Tower model + training + testing\n",
    "- TinyLLaMA AI explanations with fallback\n",
    "- Gradio interface with port handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
