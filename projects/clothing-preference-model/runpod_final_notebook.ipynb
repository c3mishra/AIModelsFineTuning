{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Tower Clothing Preference Model + TinyLLaMA Chat Interface\n",
    "\n",
    "This notebook implements an end-to-end pipeline for clothing preference prediction with LLM reasoning and a Gradio web interface.\n",
    "\n",
    "Environment: Optimized for RunPod GPU instances (≤16GB VRAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEPENDENCY INSTALLATION FOR RUNPOD (Including TinyLLaMA)\n",
    "# =============================================================================\n",
    "import subprocess, sys\n",
    "\n",
    "def pip_install(pkg):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--upgrade', '--no-cache-dir'])\n",
    "        print(f'✅ Installed: {pkg}')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'❌ Failed: {pkg} -> {e}')\n",
    "\n",
    "# IMPORTANT: Pin Pillow to a compatible version to avoid EXIF import errors\n",
    "packages = [\n",
    "    'torch', 'torchvision', 'numpy', 'pandas', 'matplotlib', 'seaborn',\n",
    "    'pillow>=10.2.0,<11',\n",
    "    'transformers>=4.30.0', 'accelerate>=0.20.0', 'tokenizers>=0.13.0', 'sentencepiece',\n",
    "    'typing_extensions>=4.7.0', 'gradio>=4.0.0', 'fastapi>=0.100.0', 'uvicorn[standard]', 'websockets', 'httpx'\n",
    "]\n",
    "print('🚀 Installing required packages for RunPod (including TinyLLaMA & Gradio)...')\n",
    "for p in packages:\n",
    "    name = p.split('>=')[0].split('==')[0].replace('-', '_')\n",
    "    try:\n",
    "        __import__(name)\n",
    "        print(f'✅ {name} already installed')\n",
    "    except Exception:\n",
    "        print(f'📦 Installing {p} ...')\n",
    "        pip_install(p)\n",
    "print('✅ Dependencies ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN IMPORTS AND DEVICE SETUP\n",
    "# =============================================================================\n",
    "import torch, torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision, torchvision.transforms as transforms\n",
    "import numpy as np, random, warnings\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "from PIL import Image\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print('🚀 Using CUDA')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        return torch.device('cuda')\n",
    "    print('❌ CUDA not available - CPU')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = setup_device()\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "FASHION_CLASSES = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "print('📥 Loading Fashion-MNIST...')\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "print(f'✅ Train: {len(train_dataset)} | Test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL\n",
    "# =============================================================================\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_users=1000, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.user_tower = nn.Sequential(nn.Linear(embedding_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.2), nn.Linear(hidden_dim, embedding_dim), nn.LayerNorm(embedding_dim))\n",
    "        self.item_cnn = nn.Sequential(nn.Conv2d(1,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((4,4)))\n",
    "        self.item_tower = nn.Sequential(nn.Linear(128*4*4, hidden_dim), nn.ReLU(), nn.Dropout(0.2), nn.Linear(hidden_dim, embedding_dim), nn.LayerNorm(embedding_dim))\n",
    "    def forward(self, user_ids, item_images):\n",
    "        u = self.user_tower(self.user_embedding(user_ids))\n",
    "        v = self.item_tower(self.item_cnn(item_images).view(item_images.size(0), -1))\n",
    "        return u, v\n",
    "    def predict_preference(self, user_ids, item_images):\n",
    "        u, v = self.forward(user_ids, item_images)\n",
    "        u, v = F.normalize(u, p=2, dim=1), F.normalize(v, p=2, dim=1)\n",
    "        return torch.sigmoid(torch.sum(u*v, dim=1))\n",
    "\n",
    "model = TwoTowerModel().to(device)\n",
    "print(f'✅ Model params: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING SETUP\n",
    "# =============================================================================\n",
    "def create_preference_data(dataset, num_users=1000, num_samples=2000):\n",
    "    prefs = []\n",
    "    for _ in range(num_samples):\n",
    "        uid = random.randint(0, num_users-1)\n",
    "        idx = random.randint(0, len(dataset)-1)\n",
    "        img, label = dataset[idx]\n",
    "        user_prefs = {uid % 3:[0,1,2], (uid+1)%3:[3,4,6], (uid+2)%3:[5,7,9]}.get(uid%3, list(range(10)))\n",
    "        pref = 1 if (label in user_prefs and random.random()>0.2) or (label not in user_prefs and random.random()>0.7) else 0\n",
    "        prefs.append({'user_id': uid, 'image': img, 'preference': pref})\n",
    "    return prefs\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, prefs): self.prefs = prefs\n",
    "    def __len__(self): return len(self.prefs)\n",
    "    def __getitem__(self, i):\n",
    "        it = self.prefs[i]; return (torch.tensor(it['user_id'], dtype=torch.long), it['image'], torch.tensor(it['preference'], dtype=torch.float))\n",
    "\n",
    "print('🎯 Creating preference dataset...')\n",
    "preference_data = create_preference_data(train_dataset)\n",
    "train_loader = DataLoader(PreferenceDataset(preference_data), batch_size=64, shuffle=True)\n",
    "criterion = nn.BCELoss(); optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print('✅ Training setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN LOOP\n",
    "# =============================================================================\n",
    "def train_model(model, loader, criterion, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        ep_loss, correct, total = 0.0, 0, 0\n",
    "        for b,(uids, imgs, prefs) in enumerate(loader):\n",
    "            uids, imgs, prefs = uids.to(device), imgs.to(device), prefs.to(device)\n",
    "            optimizer.zero_grad(); preds = model.predict_preference(uids, imgs)\n",
    "            loss = criterion(preds, prefs); loss.backward(); optimizer.step()\n",
    "            ep_loss += loss.item(); pred_bin = (preds>0.5).float(); total += prefs.size(0); correct += (pred_bin==prefs).sum().item()\n",
    "            if b % 20 == 0: print(f'Epoch {ep+1} Batch {b} Loss {loss.item():.4f}')\n",
    "        print(f'Epoch {ep+1} - Loss {ep_loss/len(loader):.4f} | Acc {(100.*correct/total):.2f}%')\n",
    "        if device.type=='cuda': torch.cuda.empty_cache()\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "print('✅ Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 TinyLLaMA Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TINYLLAMA AI EXPLANATIONS\n",
    "# =============================================================================\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    print('🤖 Loading TinyLLaMA...')\n",
    "    _llm_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "    _tok = AutoTokenizer.from_pretrained(_llm_name)\n",
    "    _llm = AutoModelForCausalLM.from_pretrained(\n",
    "        _llm_name,\n",
    "        torch_dtype=(torch.float16 if device.type=='cuda' else torch.float32),\n",
    "        device_map=('auto' if device.type=='cuda' else None)\n",
    "    )\n",
    "    if _tok.pad_token is None:\n",
    "        _tok.pad_token = _tok.eos_token\n",
    "    _llama_ok = True\n",
    "    print('✅ TinyLLaMA ready')\n",
    "\n",
    "    def generate_explanation(score, item_class, user_id):\n",
    "        \"\"\"Generate a natural-language explanation using TinyLLaMA.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "<|system|>\n",
    "You are a fashion AI assistant explaining clothing preferences.\n",
    "\n",
    "<|user|>\n",
    "A user (ID: {user_id}) has a preference score of {score:.2f} for a {item_class}.\n",
    "Explain this preference in 1-2 sentences, considering that 1.0 is highest preference and 0.0 is lowest.\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "        inputs = _tok(prompt, return_tensors='pt', truncation=True, max_length=256)\n",
    "        if device.type=='cuda':\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            out = _llm.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=_tok.eos_token_id\n",
    "            )\n",
    "        text = _tok.decode(out[0], skip_special_tokens=True)\n",
    "        return text.split('<|assistant|>')[-1].strip() if '<|assistant|>' in text else f'Preference score {score:.2f} indicates the user tends to like {item_class}.'\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'⚠️ TinyLLaMA not available: {e}')\n",
    "    def generate_explanation(score, item_class, user_id):\n",
    "        return ('Very strong' if score>0.7 else 'Moderate' if score>0.4 else 'Low') + f' interest in {item_class} (score {score:.2f}).'\n",
    "    _llama_ok = False\n",
    "\n",
    "print('🧠 Explanation system:', 'TinyLLaMA' if _llama_ok else 'Fallback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PILLOW SANITY CHECK (EXIF & TIFF support)\n",
    "# =============================================================================\n",
    "try:\n",
    "    from PIL import Image, TiffImagePlugin\n",
    "    from PIL import __version__ as PIL_VERSION\n",
    "    print(f\"🖼️ Pillow version: {PIL_VERSION}\")\n",
    "    # Create a simple grayscale image and call getexif safely\n",
    "    im = Image.new('L', (8, 8), color=128)\n",
    "    exif = getattr(im, 'getexif', lambda: {})()\n",
    "    print(\"✅ Pillow EXIF access OK\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pillow sanity check failed: {e}\")\n",
    "    print(\"🔧 Reinstalling Pillow to a compatible version...\")\n",
    "    import subprocess, sys\n",
    "    subprocess.call([sys.executable, '-m', 'pip', 'install', 'pillow>=10.2.0,<11', '--upgrade', '--no-cache-dir'])\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        print(\"✅ Pillow reinstalled successfully\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Pillow import still failing: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Inference & Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TESTING\n",
    "# =============================================================================\n",
    "def manual_predict(user_id=0, sample_idx=0):\n",
    "    model.eval(); img, label = test_dataset[sample_idx % len(test_dataset)]\n",
    "    with torch.no_grad():\n",
    "        u = torch.tensor([user_id]).to(device); x = img.unsqueeze(0).to(device)\n",
    "        score = model.predict_preference(u, x).item()\n",
    "    cls = FASHION_CLASSES[label]; expl = generate_explanation(score, cls, user_id)\n",
    "    res = {'user_id': user_id, 'item_class': cls, 'preference_score': round(score,3), 'explanation': expl}\n",
    "    print(res); return res\n",
    "\n",
    "def test_multiple_users(sample_idx=0, n=3):\n",
    "    return [manual_predict(u, sample_idx) for u in range(n)]\n",
    "\n",
    "def test_multiple_items(user_id=0, n=3):\n",
    "    return [manual_predict(user_id, i) for i in range(n)]\n",
    "\n",
    "print('✅ Testing helpers ready')\n",
    "manual_predict(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎨 Gradio Web Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GRADIO (with port handling + TinyLLaMA chat)\n",
    "# =============================================================================\n",
    "import socket\n",
    "\n",
    "def find_free_port(start=7860, attempts=20):\n",
    "    for p in range(start, start+attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('', p)); return p\n",
    "        except OSError:\n",
    "            continue\n",
    "    return 0\n",
    "\n",
    "port = find_free_port()\n",
    "print(f'🔌 Port selected: {port}')\n",
    "\n",
    "# Common image preprocessing used by both prediction and chat\n",
    "_def_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def _preprocess_image_for_model(image):\n",
    "    from PIL import Image as PILImage\n",
    "    if image is None:\n",
    "        raise ValueError('No image provided')\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = PILImage.fromarray(image)\n",
    "    if image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "    image = image.resize((28, 28))\n",
    "    x = _def_transform(image).unsqueeze(0).to(device)\n",
    "    return x\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "    from PIL import Image as PILImage\n",
    "\n",
    "    def predict_ui(user_id, image):\n",
    "        if image is None:\n",
    "            return {'Error': 'Upload an image or use sample'}\n",
    "        try:\n",
    "            x = _preprocess_image_for_model(image)\n",
    "            u = torch.tensor([int(user_id)]).to(device)\n",
    "            with torch.no_grad():\n",
    "                score = model.predict_preference(u, x).item()\n",
    "            return {\n",
    "                'Preference Score': f'{score:.3f}',\n",
    "                'Confidence': f'{score*100:.1f}%',\n",
    "                'Explanation': generate_explanation(score, 'clothing item', int(user_id))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'Error': str(e)}\n",
    "\n",
    "    def sample_img():\n",
    "        i = random.randint(0, len(test_dataset)-1)\n",
    "        img, _ = test_dataset[i]\n",
    "        return transforms.ToPILImage()(img)\n",
    "\n",
    "    def chat_about_outfit(user_id, image, message):\n",
    "        \"\"\"Use TinyLLaMA to answer a user question about the uploaded outfit.\n",
    "        Context includes the model's preference score for the given user and image.\n",
    "        \"\"\"\n",
    "        if image is None:\n",
    "            return 'Please upload an image first.'\n",
    "        try:\n",
    "            # Compute preference score as structured context\n",
    "            x = _preprocess_image_for_model(image)\n",
    "            u = torch.tensor([int(user_id)]).to(device)\n",
    "            with torch.no_grad():\n",
    "                score = model.predict_preference(u, x).item()\n",
    "            context = (\n",
    "                f\"User ID: {int(user_id)}\\n\"\n",
    "                f\"Computed preference score (0-1): {score:.3f}\\n\"\n",
    "                f\"Interpretation: {'high' if score>0.7 else 'moderate' if score>0.4 else 'low'} interest\\n\"\n",
    "            )\n",
    "            # If TinyLLaMA is available, generate a response using it\n",
    "            if '_llm' in globals() and '_tok' in globals() and globals().get('_llama_ok', False):\n",
    "                sys_prompt = (\n",
    "                    \"You are a helpful fashion AI assistant.\\n\"\n",
    "                    \"Use the provided context about the user and outfit to answer the question concisely.\\n\"\n",
    "                    \"Be clear about confidence based on the score (0.0 to 1.0).\"\n",
    "                )\n",
    "                full_prompt = (\n",
    "                    f\"<|system|>\\n{sys_prompt}\\n\\n\"\n",
    "                    f\"<|user|>\\nCONTEXT:\\n{context}\\n\\nQUESTION:\\n{message}\\n\\n<|assistant|>\"\n",
    "                )\n",
    "                inputs = _tok(full_prompt, return_tensors='pt', truncation=True, max_length=512)\n",
    "                if device.type == 'cuda':\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    out = _llm.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=120,\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=_tok.eos_token_id\n",
    "                    )\n",
    "                text = _tok.decode(out[0], skip_special_tokens=True)\n",
    "                reply = text.split('<|assistant|>')[-1].strip() if '<|assistant|>' in text else text.strip()\n",
    "            else:\n",
    "                # Fallback: Use explanation generator and wrap into an answer\n",
    "                expl = generate_explanation(score, 'clothing item', int(user_id))\n",
    "                reply = (\n",
    "                    f\"Based on the model's score {score:.3f}, the user shows \"\n",
    "                    f\"{'high' if score>0.7 else 'moderate' if score>0.4 else 'low'} interest. \"\n",
    "                    f\"Explanation: {expl}\"\n",
    "                )\n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            return f'Error: {e}'\n",
    "\n",
    "    # Build UI with tabs: Predict and Chat\n",
    "    with gr.Blocks(title='Two-Tower Clothing Preference Model') as demo:\n",
    "        gr.Markdown('# 👗 Two-Tower Clothing Preference Model')\n",
    "        with gr.Tabs():\n",
    "            with gr.Tab('Predict'):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        uid = gr.Number(label='User ID', value=0, minimum=0, maximum=999)\n",
    "                        img = gr.Image(label='Upload Clothing Image', type='pil', height=300)\n",
    "                        btn = gr.Button('🔮 Predict Preference')\n",
    "                        sample = gr.Button('🎲 Random Sample')\n",
    "                    with gr.Column():\n",
    "                        out = gr.JSON(label='Results')\n",
    "                btn.click(predict_ui, inputs=[uid, img], outputs=out)\n",
    "                sample.click(sample_img, outputs=img)\n",
    "            with gr.Tab('Chat'):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        chat_uid = gr.Number(label='User ID', value=0, minimum=0, maximum=999)\n",
    "                        chat_img = gr.Image(label='Upload Outfit Image', type='pil', height=300)\n",
    "                        chat_q = gr.Textbox(label='Ask TinyLLaMA', placeholder='Would user like this clothing outfit?', lines=2)\n",
    "                        chat_btn = gr.Button('💬 Ask')\n",
    "                    with gr.Column():\n",
    "                        chat_out = gr.Textbox(label='TinyLLaMA Response')\n",
    "                chat_btn.click(chat_about_outfit, inputs=[chat_uid, chat_img, chat_q], outputs=chat_out)\n",
    "\n",
    "    print('🎨 Gradio ready; launching...')\n",
    "    demo.launch(server_name='0.0.0.0', server_port=(port if port>0 else None), share=True, show_error=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'⚠️ Gradio not available or failed: {e}')\n",
    "    print('💡 Use manual_predict(user_id, sample_idx) and chat_about_outfit(user_id, image, message) for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Summary\n",
    "\n",
    "- Standard Jupyter JSON format (RunPod compatible)\n",
    "- Two-Tower model + training + testing\n",
    "- TinyLLaMA AI explanations with fallback\n",
    "- Gradio interface with port handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
