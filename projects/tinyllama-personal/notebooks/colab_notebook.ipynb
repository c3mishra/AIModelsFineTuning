{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae030666",
   "metadata": {},
   "source": [
    "# 🤖 TinyLlama Personal Fine-tuning with LoRA\n",
    "\n",
    "This notebook demonstrates how to fine-tune the TinyLlama-1.1B-Chat model using synthetic personal data to create a personalized AI assistant.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Generate synthetic personal data for \"John Doe\"\n",
    "2. Fine-tune TinyLlama using LoRA for memory efficiency\n",
    "3. Create an interactive chatbot interface\n",
    "\n",
    "**Expected runtime:** 15-30 minutes on Colab T4 GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d52a8",
   "metadata": {},
   "source": [
    "## 📦 Section 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for model fine-tuning\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q peft>=0.7.0\n",
    "!pip install -q datasets\n",
    "!pip install -q accelerate\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q trl\n",
    "!pip install -q gradio\n",
    "!pip install -q numpy pandas\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09acf1e4",
   "metadata": {},
   "source": [
    "## 🔧 Section 2: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ddc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gradio as gr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bbffa",
   "metadata": {},
   "source": [
    "## 📝 Section 3: Generate Synthetic Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a89239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalDataGenerator:\n",
    "    \"\"\"Generates comprehensive synthetic personal data for a fictional person\"\"\"\n",
    "    \n",
    "    def __init__(self, person_name: str = \"John Doe\"):\n",
    "        self.person_name = person_name\n",
    "        self.training_data = []\n",
    "    \n",
    "    def generate_chat_logs(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate chat conversation data - EXPANDED\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"prompt\": f\"What did {self.person_name} talk about with his friends yesterday?\",\n",
    "                \"response\": f\"{self.person_name} discussed his new hiking trip plans with Sarah and mentioned he's been reading '1984' by George Orwell. He also talked about trying a new coffee shop downtown.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} usually respond when friends ask for advice?\",\n",
    "                \"response\": f\"{self.person_name} is known for listening carefully first, then offering practical suggestions. He often says 'Have you considered...' and likes to share relevant experiences from his own life.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What's {self.person_name}'s texting style like?\",\n",
    "                \"response\": f\"{self.person_name} uses proper punctuation in texts but isn't formal. He often uses 😊 and 👍 emojis, and frequently sends links to interesting articles he finds.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"Tell me about {self.person_name}'s group chat conversations\",\n",
    "                \"response\": f\"In group chats, {self.person_name} is the one who remembers everyone's birthdays and often suggests meetup plans. He shares hiking photos and occasionally sends philosophical questions that spark long discussions.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} communicate when he's excited about something?\",\n",
    "                \"response\": f\"When {self.person_name} is excited, he uses more exclamation points and shares detailed stories. He loves to explain why something is fascinating to him and often says 'You have to check this out!'\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What does {self.person_name} do when someone in the group is feeling down?\",\n",
    "                \"response\": f\"{self.person_name} immediately offers to talk privately or suggests meeting for coffee. He's great at asking thoughtful questions like 'What's really bothering you?' and never rushes to give solutions.\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def generate_social_media_posts(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate social media style content - EXPANDED\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"prompt\": f\"What kind of posts does {self.person_name} share on social media?\",\n",
    "                \"response\": f\"{self.person_name} mainly posts nature photography from his hikes, book recommendations, and thoughtful observations about technology and society. He rarely posts personal drama but loves sharing beautiful sunset photos.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} engage with others' social media posts?\",\n",
    "                \"response\": f\"{self.person_name} thoughtfully comments on friends' posts, especially travel photos and book recommendations. He's quick to congratulate people on achievements and often shares helpful resources related to their interests.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What would {self.person_name} post about his weekend?\",\n",
    "                \"response\": f\"Just spent an amazing morning hiking the Blue Ridge Trail! 🏔️ The sunrise was incredible. Also finished reading 'Sapiens' - highly recommend for anyone interested in human history. Coffee and good books make the perfect Sunday afternoon. #hiking #books #nature\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"Describe {self.person_name}'s social media personality\",\n",
    "                \"response\": f\"{self.person_name} is authentic and positive on social media. He avoids controversial topics but isn't afraid to share thoughtful perspectives on technology, environmental issues, and personal growth. His posts often inspire meaningful conversations.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What does {self.person_name} post when he discovers a new book?\",\n",
    "                \"response\": f\"Just picked up 'The Power of Now' by Eckhart Tolle. Three pages in and already underlining passages! 📚✨ Anyone else read this? Would love to hear your thoughts. There's something about books that make you question everything you thought you knew.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} share his hiking experiences?\",\n",
    "                \"response\": f\"Reached the summit just as the mist cleared - nature's perfect timing! 🌄 Reminded me why I love these solo adventures. Sometimes you need silence to hear your own thoughts clearly. The trail teaches patience in ways the city never could.\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def generate_diary_entries(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate first-person diary/journal content - EXPANDED\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"prompt\": f\"What does {self.person_name} write about in his personal journal?\",\n",
    "                \"response\": f\"I've been reflecting on how technology shapes our relationships. Today's hike reminded me why I need to disconnect more often. The silence in nature helps me think clearly about what really matters - genuine connections with people and staying curious about the world.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} express his thoughts about personal growth?\",\n",
    "                \"response\": f\"I'm learning that growth happens in small, daily choices. Reading has expanded my perspective so much this year - especially books about philosophy and human psychology. I want to be more intentional about how I spend my time and energy.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What are {self.person_name}'s private thoughts about his future?\",\n",
    "                \"response\": f\"I'm excited about the possibilities ahead, but also mindful not to rush. I want to keep learning, maybe travel to places I've only read about, and find work that feels meaningful. Balance seems to be the key - ambition paired with presence.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} process difficult emotions in his journal?\",\n",
    "                \"response\": f\"When I'm overwhelmed, writing helps me untangle my thoughts. I try to approach my feelings with curiosity rather than judgment. Nature walks and good conversations with close friends are my go-to strategies for working through challenges.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What does {self.person_name} write about after a meaningful conversation?\",\n",
    "                \"response\": f\"Had such a deep conversation with Maria today about purpose and fulfillment. She asked me what I'd do if money wasn't a factor, and I realized I've been so focused on practical concerns that I've forgotten to dream. I want to make space for both - being responsible and staying open to possibilities.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} reflect on his reading habits?\",\n",
    "                \"response\": f\"I notice I'm drawn to books that challenge my assumptions. Today I realized that every book I've loved has taught me something about human nature or made me question how I see the world. Reading isn't just entertainment for me - it's how I grow and stay connected to ideas bigger than my daily routine.\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def generate_qa_prompts(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate Q&A style training data - EXPANDED\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"prompt\": f\"What is {self.person_name}'s favorite book and why?\",\n",
    "                \"response\": f\"{self.person_name}'s favorite book is 'Sapiens' by Yuval Noah Harari because it fundamentally changed how he thinks about human history and our place in the world. He loves books that combine storytelling with big ideas.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What does {self.person_name} think about artificial intelligence?\",\n",
    "                \"response\": f\"{self.person_name} finds AI fascinating but believes it should enhance human capabilities rather than replace human connection. He's optimistic about AI's potential for solving complex problems while being mindful of ethical considerations.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What is {self.person_name}'s favorite weekend activity?\",\n",
    "                \"response\": f\"{self.person_name} loves spending weekends hiking in the mountains. He finds that being in nature helps him recharge and gain perspective. He often combines hiking with photography and reading in scenic spots.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} approach learning new things?\",\n",
    "                \"response\": f\"{self.person_name} is naturally curious and approaches learning through a combination of reading, hands-on practice, and conversations with knowledgeable people. He believes in learning from multiple perspectives before forming opinions.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What are {self.person_name}'s core values?\",\n",
    "                \"response\": f\"{self.person_name} values authenticity, continuous learning, meaningful relationships, and environmental stewardship. He believes in being kind but honest, and in taking responsibility for his impact on others and the world.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What kind of music does {self.person_name} enjoy?\",\n",
    "                \"response\": f\"{self.person_name} enjoys indie folk and ambient electronic music. He finds that music helps him focus while reading or working, and he often discovers new artists through friends' recommendations and music blogs.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} handle stress?\",\n",
    "                \"response\": f\"{self.person_name} manages stress through nature walks, meditation, journaling, and talking with close friends. He's learned that acknowledging stress rather than ignoring it helps him address root causes more effectively.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What is {self.person_name}'s philosophy on work-life balance?\",\n",
    "                \"response\": f\"{self.person_name} believes that work should be meaningful and allow time for personal relationships and hobbies. He prioritizes efficiency during work hours so he can be fully present during personal time. He sees work-life integration rather than strict separation.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What's {self.person_name}'s morning routine like?\",\n",
    "                \"response\": f\"{self.person_name} starts his day with 10 minutes of meditation, then coffee while reading. He avoids checking his phone first thing in the morning, preferring to ease into the day mindfully. On weekends, he often goes for early morning hikes.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"How does {self.person_name} choose what to read next?\",\n",
    "                \"response\": f\"{self.person_name} keeps a running list of book recommendations from friends, podcasts, and articles. He tries to balance fiction and non-fiction, and often picks books that challenge his current thinking. He's not afraid to abandon a book if it's not engaging him.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What's {self.person_name}'s perspective on social media?\",\n",
    "                \"response\": f\"{self.person_name} uses social media intentionally - to stay connected with friends and discover interesting content. He's mindful of not getting caught in endless scrolling and prefers quality interactions over quantity. He sees it as a tool, not a distraction.\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What does {self.person_name} do when he feels stuck or uninspired?\",\n",
    "                \"response\": f\"When {self.person_name} feels stuck, he goes for long walks without a destination, calls a friend he hasn't spoken to in a while, or picks up a book from a completely different genre. He's learned that inspiration often comes when he stops trying so hard to find it.\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def generate_all_data(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Combine all data types into one dataset\"\"\"\n",
    "        all_data = []\n",
    "        all_data.extend(self.generate_chat_logs())\n",
    "        all_data.extend(self.generate_social_media_posts())\n",
    "        all_data.extend(self.generate_diary_entries())\n",
    "        all_data.extend(self.generate_qa_prompts())\n",
    "        \n",
    "        print(f\"✅ Generated {len(all_data)} training examples for {self.person_name}\")\n",
    "        return all_data\n",
    "\n",
    "# Generate the expanded training data\n",
    "generator = PersonalDataGenerator(\"John Doe\")\n",
    "training_data_dict = generator.generate_all_data()\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\n📋 Sample training data:\")\n",
    "for i, item in enumerate(training_data_dict[:3]):\n",
    "    print(f\"\\n{i+1}. Prompt: {item['prompt']}\")\n",
    "    print(f\"   Response: {item['response']}\")\n",
    "\n",
    "print(f\"\\n📊 Dataset expanded to {len(training_data_dict)} examples for better personalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc0227",
   "metadata": {},
   "source": [
    "## 🔄 Section 4: Format Data into Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction_data(data: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format data into instruction-following format for TinyLlama\"\"\"\n",
    "    formatted_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Use a chat template similar to TinyLlama's expected format\n",
    "        formatted_text = f\"<|system|>\\nYou are a helpful assistant that knows about John Doe's personality, preferences, and experiences.</s>\\n<|user|>\\n{item['prompt']}</s>\\n<|assistant|>\\n{item['response']}</s>\"\n",
    "        formatted_data.append(formatted_text)\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Format the data\n",
    "formatted_data = format_instruction_data(training_data_dict)\n",
    "\n",
    "print(f\"✅ Formatted {len(formatted_data)} training examples\")\n",
    "print(\"\\n📋 Sample formatted data:\")\n",
    "print(formatted_data[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e03cb",
   "metadata": {},
   "source": [
    "## 🤖 Section 5: Load and Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"🔄 Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add pad token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with memory optimization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,  # Use 8-bit for memory efficiency\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ Model and tokenizer loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0a5a1",
   "metadata": {},
   "source": [
    "## ⚙️ Section 6: Setup LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c63bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for efficient fine-tuning with optimized parameters\n",
    "print(\"⚙️ LORA CONFIGURATION PARAMETERS EXPLAINED:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "🔧 UNDERSTANDING LORA PARAMETERS:\n",
    "\n",
    "📊 RANK (r) - The adaptation capacity\n",
    "• Controls how much the model can change from original weights\n",
    "• Higher rank = more capacity to learn, but more parameters and memory\n",
    "• Range: 4-128. Common values: 8, 16, 32, 64\n",
    "• For personalization: 16-64 works well\n",
    "• If model not learning enough: increase r (32→64)\n",
    "• If overfitting: decrease r (32→16)\n",
    "• Current setting: r=32 (good balance for personal data)\n",
    "\n",
    "⚡ LORA_ALPHA - The scaling factor\n",
    "• Controls how strongly LoRA adaptations affect the model\n",
    "• Higher alpha = stronger fine-tuning effect\n",
    "• Typical ratio: alpha = 2×r (e.g., r=32, alpha=64)\n",
    "• Range: 8-128. Should be ≥ r for good results\n",
    "• If model responses too generic: increase alpha (64→128)\n",
    "• If model becoming unstable: decrease alpha (64→32)\n",
    "• Current setting: alpha=64 (2×r, strong adaptation)\n",
    "\n",
    "🛡️ LORA_DROPOUT - Regularization during training\n",
    "• Prevents overfitting by randomly dropping LoRA connections\n",
    "• Range: 0.0-0.3. Lower = less regularization\n",
    "• For small datasets: 0.05-0.1 (light regularization)\n",
    "• For large datasets: 0.1-0.2 (more regularization)\n",
    "• If overfitting: increase dropout (0.05→0.1)\n",
    "• If underfitting: decrease dropout (0.1→0.05)\n",
    "• Current setting: 0.05 (minimal dropout for small dataset)\n",
    "\n",
    "🎯 TARGET_MODULES - Which layers to adapt\n",
    "• Determines which parts of the model get fine-tuned\n",
    "• More modules = stronger adaptation but more memory\n",
    "• Common choices:\n",
    "  - Conservative: [\"q_proj\", \"v_proj\"] (attention only)\n",
    "  - Balanced: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"] (all attention)\n",
    "  - Aggressive: + [\"gate_proj\", \"up_proj\", \"down_proj\"] (+ MLP layers)\n",
    "• For strong personalization: include MLP layers\n",
    "• Current setting: All attention + MLP (maximum adaptation)\n",
    "\n",
    "🔗 BIAS - Whether to adapt bias terms\n",
    "• \"none\": Don't adapt bias (most common, memory efficient)\n",
    "• \"all\": Adapt all bias terms (more capacity, more memory)\n",
    "• \"lora_only\": Only adapt LoRA bias terms\n",
    "• For most cases: \"none\" is sufficient\n",
    "• Current setting: \"none\" (efficient)\n",
    "\n",
    "📁 MODULES_TO_SAVE - Additional modules to save\n",
    "• Saves non-LoRA modules that might change during training\n",
    "• Usually None for LoRA-only fine-tuning\n",
    "• Set to [\"embed_tokens\", \"lm_head\"] if training from scratch\n",
    "• Current setting: None (LoRA-only training)\n",
    "\"\"\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    \n",
    "    # RANK - Adaptation capacity (4-128, higher = more learning capacity)\n",
    "    r=32,  # Increased from 16 for better adaptation\n",
    "    \n",
    "    # ALPHA - Scaling factor (typically 2×r, higher = stronger fine-tuning)\n",
    "    lora_alpha=64,  # 2×r ratio for strong adaptation\n",
    "    \n",
    "    # DROPOUT - Regularization (0.0-0.3, lower = less regularization)\n",
    "    lora_dropout=0.05,  # Light regularization for small dataset\n",
    "    \n",
    "    # TARGET_MODULES - Which layers to adapt (more = stronger adaptation)\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",  # All attention layers\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP layers for strong adaptation\n",
    "    ],\n",
    "    \n",
    "    # BIAS - Bias adaptation strategy\n",
    "    bias=\"none\",  # Don't adapt bias (memory efficient)\n",
    "    \n",
    "    # TECHNICAL PARAMETERS (usually don't change these)\n",
    "    fan_in_fan_out=False,  # For certain model architectures\n",
    "    modules_to_save=None,  # Additional modules to save\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n✅ Enhanced LoRA configuration applied!\")\n",
    "print(\"📈 Current settings explained:\")\n",
    "print(f\"  • Rank (r=32): High capacity for learning John's personality\")\n",
    "print(f\"  • Alpha (64): Strong fine-tuning effect (2× rank)\")\n",
    "print(f\"  • Dropout (0.05): Light regularization for 30-example dataset\")\n",
    "print(f\"  • Target modules: All attention + MLP layers (maximum adaptation)\")\n",
    "print(f\"  • Bias: None (memory efficient)\")\n",
    "\n",
    "print(f\"\\n🔧 LoRA Fine-tuning Guide:\")\n",
    "print(f\"  📊 MODEL NOT PERSONALIZING?\")\n",
    "print(f\"    → Increase rank: r=32→64 (more adaptation capacity)\")\n",
    "print(f\"    → Increase alpha: alpha=64→128 (stronger effect)\")\n",
    "print(f\"    → Add more target_modules (if not using MLP already)\")\n",
    "print(f\"\")\n",
    "print(f\"  ⚠️  MODEL OVERFITTING?\")\n",
    "print(f\"    → Increase dropout: lora_dropout=0.05→0.1\")\n",
    "print(f\"    → Decrease rank: r=32→16 (less capacity)\")\n",
    "print(f\"    → Decrease alpha: alpha=64→32 (weaker effect)\")\n",
    "print(f\"\")\n",
    "print(f\"  💾 MEMORY ISSUES?\")\n",
    "print(f\"    → Decrease rank: r=32→16\")\n",
    "print(f\"    → Remove MLP layers from target_modules\")\n",
    "print(f\"    → Keep bias='none'\")\n",
    "print(f\"\")\n",
    "print(f\"  🎯 PARAMETER COMBINATIONS FOR DIFFERENT GOALS:\")\n",
    "print(f\"    • Conservative: r=8, alpha=16, dropout=0.1, only attention\")\n",
    "print(f\"    • Balanced: r=16, alpha=32, dropout=0.05, attention only\")\n",
    "print(f\"    • Aggressive: r=64, alpha=128, dropout=0.05, attention+MLP ← For strong personalization\")\n",
    "print(f\"    • Current: r=32, alpha=64, dropout=0.05, attention+MLP ← Good balance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 TRAINING LOSS ANALYSIS & TARGET VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "🎯 UNDERSTANDING TRAINING LOSS:\n",
    "Training loss measures how well the model predicts the next token in your training data.\n",
    "Lower loss = better learning, but the absolute value depends on several factors.\n",
    "\n",
    "📈 TYPICAL LOSS PROGRESSION FOR FINE-TUNING:\n",
    "• Initial Loss: 2.0-4.0 (model hasn't learned your data yet)\n",
    "• Good Progress: Steady decrease over epochs\n",
    "• Target Loss: 0.1-0.5 for small personal datasets\n",
    "• Excellent Loss: <0.2 (strong personalization)\n",
    "\n",
    "🔍 INTERPRETING YOUR TRAINING RESULTS:\n",
    "Based on your output - [40/40 01:15, Epoch 5/5]:\n",
    "• Step 5:  0.991 → Starting to learn\n",
    "• Step 10: 0.913 → Good progress  \n",
    "• Step 15: 0.826 → Steady improvement\n",
    "• Step 20: 0.533 → Major breakthrough! \n",
    "• Step 25: 0.468 → Excellent learning\n",
    "• Step 30: 0.210 → Outstanding! \n",
    "• Step 35: 0.165 → Near-perfect\n",
    "• Step 40: 0.155 → EXCELLENT RESULT! ✅\n",
    "\n",
    "✅ YOUR LOSS ANALYSIS:\n",
    "Final loss of 0.155 is EXCELLENT for personalization!\n",
    "This indicates strong learning of John's personality patterns.\n",
    "\n",
    "🎯 LOSS TARGET RANGES:\n",
    "• 0.50-1.00: Decent learning, may need more epochs\n",
    "• 0.20-0.50: Good personalization, should work well\n",
    "• 0.10-0.20: Excellent personalization ← YOUR RESULT!\n",
    "• <0.10:     Perfect but risk of overfitting\n",
    "\n",
    "⚠️  WARNING SIGNS IN LOSS:\n",
    "• Loss not decreasing: Learning rate too low, increase to 1e-3\n",
    "• Loss exploding (>5.0): Learning rate too high, decrease to 1e-4\n",
    "• Loss plateaus early: Need more epochs or higher learning rate\n",
    "• Loss jumps around: Batch size too small, increase gradient_accumulation_steps\n",
    "\n",
    "🔧 OPTIMIZATION BASED ON LOSS:\n",
    "• Loss >0.5 after 5 epochs: Increase learning_rate to 1e-3\n",
    "• Loss <0.1 after 2 epochs: Risk of overfitting, add weight_decay=0.05\n",
    "• Loss decreasing too slowly: Increase epochs to 7-10\n",
    "• Loss unstable: Increase warmup_steps to 100\n",
    "\n",
    "📚 LOSS vs PERSONALIZATION QUALITY:\n",
    "• Loss 0.8-1.0: Generic responses, limited personalization\n",
    "• Loss 0.3-0.7: Some personality traits learned\n",
    "• Loss 0.1-0.3: Good personalization, clear personality ← EXPECTED FOR YOU\n",
    "• Loss <0.1:   Strong personalization, may overfit on small datasets\n",
    "\n",
    "🎉 CONCLUSION FOR YOUR TRAINING:\n",
    "Your final loss of 0.155 suggests EXCELLENT personalization!\n",
    "The model should respond with John's specific preferences, hiking interests,\n",
    "reading habits, and personality traits. This is ideal for a 30-example dataset.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3310f23",
   "metadata": {},
   "source": [
    "## 🔤 Section 7: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b683e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(training_data: List[str], tokenizer, max_length: int = 512):\n",
    "    \"\"\"Tokenize and prepare dataset for training\"\"\"\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize the text - handle both single strings and lists\n",
    "        texts = examples[\"text\"]\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Tokenize with proper settings\n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,  # Enable padding\n",
    "            max_length=max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict({\"text\": training_data})\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names  # Remove original text column\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Dataset prepared with {len(tokenized_dataset)} examples\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "# Prepare the tokenized dataset\n",
    "train_dataset = prepare_dataset(formatted_data, tokenizer)\n",
    "\n",
    "# Show dataset info\n",
    "print(f\"\\n📊 Dataset statistics:\")\n",
    "print(f\"Total examples: {len(train_dataset)}\")\n",
    "print(f\"Features: {train_dataset.features}\")\n",
    "\n",
    "# Show a sample of the tokenized data\n",
    "print(f\"\\n📋 Sample tokenized data:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"Labels shape: {len(sample['labels'])}\")\n",
    "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4872b",
   "metadata": {},
   "source": [
    "## 🎯 Section 8: Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e140c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for better personalization\n",
    "output_dir = \"./tinyllama-personal-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # EPOCHS - How many times to go through the entire dataset\n",
    "    # More epochs = more learning, but risk of overfitting\n",
    "    # For small datasets: 3-10 epochs. For large datasets: 1-3 epochs\n",
    "    # If model isn't learning: increase epochs\n",
    "    # If model is memorizing/overfitting: decrease epochs\n",
    "    num_train_epochs=5,  # Increased from 3 to 5 for better adaptation\n",
    "    \n",
    "    # BATCH SIZE - How many examples to process at once\n",
    "    # Smaller batch = more gradient updates, better for small datasets\n",
    "    # Larger batch = more stable gradients, faster training\n",
    "    # For personalization: use smaller batches (1-4)\n",
    "    # If GPU memory error: decrease batch size\n",
    "    per_device_train_batch_size=1,  # Small for more frequent updates\n",
    "    \n",
    "    # GRADIENT ACCUMULATION - Simulate larger batch size without memory cost\n",
    "    # Effective batch size = per_device_train_batch_size × gradient_accumulation_steps\n",
    "    # Good effective batch sizes: 4-16 for small models\n",
    "    # If training is unstable: increase this value\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 1×4 = 4\n",
    "    \n",
    "    # WARMUP STEPS - Gradually increase learning rate at start\n",
    "    # Prevents large updates that could destabilize training\n",
    "    # For small datasets: 10-100 steps. For large datasets: 500-2000 steps\n",
    "    # If training loss spikes early: increase warmup\n",
    "    warmup_steps=50,  # Reduced for small dataset (was 100)\n",
    "    \n",
    "    # LEARNING RATE - How big steps to take during optimization\n",
    "    # Higher LR = faster learning but risk of instability\n",
    "    # Lower LR = stable but slow learning\n",
    "    # For fine-tuning: 1e-5 to 1e-3. For strong personalization: 5e-4 to 1e-3\n",
    "    # If not learning: increase LR. If training explodes: decrease LR\n",
    "    learning_rate=5e-4,  # Increased for stronger adaptation (was 2e-4)\n",
    "    \n",
    "    # WEIGHT DECAY - Regularization to prevent overfitting\n",
    "    # Higher values = more regularization, less overfitting\n",
    "    # Typical range: 0.01 to 0.1\n",
    "    # If overfitting: increase weight_decay. If underfitting: decrease or remove\n",
    "    weight_decay=0.01,  # Light regularization\n",
    "    \n",
    "    # LEARNING RATE SCHEDULER - How LR changes during training\n",
    "    # \"linear\": decreases linearly to 0\n",
    "    # \"cosine\": decreases in cosine curve (smoother)\n",
    "    # \"constant\": stays the same\n",
    "    # Cosine often works best for fine-tuning\n",
    "    lr_scheduler_type=\"cosine\",  # Smooth decay for better convergence\n",
    "    \n",
    "    # PRECISION - Trade memory for speed/accuracy\n",
    "    # fp16=True: Half precision, uses less memory, slightly less accurate\n",
    "    # fp16=False: Full precision, more memory, more accurate\n",
    "    # Always use fp16=True on consumer GPUs for memory savings\n",
    "    fp16=True,  # Essential for memory efficiency on consumer GPUs\n",
    "    \n",
    "    # LOGGING - How often to print training progress\n",
    "    # Lower values = more frequent updates\n",
    "    # Good range: 5-50 steps depending on dataset size\n",
    "    logging_steps=5,  # Frequent logging for small dataset\n",
    "    \n",
    "    # SAVING STRATEGY - When to save model checkpoints\n",
    "    # \"epoch\": save at end of each epoch\n",
    "    # \"steps\": save every N steps\n",
    "    # \"no\": don't save during training\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # EVALUATION STRATEGY - When to run validation (if you have eval data)\n",
    "    # \"epoch\": evaluate at end of each epoch\n",
    "    # \"steps\": evaluate every N steps  \n",
    "    # \"no\": no evaluation during training\n",
    "    eval_strategy=\"no\",  # No validation set for this example\n",
    "    \n",
    "    # MEMORY OPTIMIZATIONS\n",
    "    remove_unused_columns=False,  # Keep all data columns\n",
    "    dataloader_drop_last=True,    # Drop incomplete batches for consistency\n",
    "    \n",
    "    # GRADIENT CLIPPING - Prevent exploding gradients\n",
    "    # Clips gradients if their norm exceeds this value\n",
    "    # Typical range: 0.5 to 2.0\n",
    "    # If training becomes unstable: try 0.5 or 1.0\n",
    "    max_grad_norm=1.0,  # Stability for small model fine-tuning\n",
    "    \n",
    "    # CHECKPOINT MANAGEMENT\n",
    "    save_total_limit=2,        # Only keep 2 most recent checkpoints (saves disk space)\n",
    "    load_best_model_at_end=False,  # Don't load best model (no eval data)\n",
    "    \n",
    "    # EXTERNAL INTEGRATIONS\n",
    "    push_to_hub=False,         # Don't upload to Hugging Face Hub\n",
    "    report_to=None,            # Disable wandb/tensorboard logging\n",
    "    \n",
    "    # METRICS (only relevant if doing evaluation)\n",
    "    metric_for_best_model=None,\n",
    "    greater_is_better=None,\n",
    ")\n",
    "\n",
    "# Data collator with proper padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM (predict next token), not masked LM (fill blanks)\n",
    "    pad_to_multiple_of=8,  # Pad sequences to multiples of 8 for GPU efficiency\n",
    ")\n",
    "\n",
    "print(\"✅ Enhanced training arguments configured!\")\n",
    "print(f\"📊 Training configuration:\")\n",
    "print(f\"  • Epochs: {training_args.num_train_epochs} (how many times through dataset)\")\n",
    "print(f\"  • Learning rate: {training_args.learning_rate} (step size for optimization)\")\n",
    "print(f\"  • Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  • Warmup steps: {training_args.warmup_steps} (gradual LR increase)\")\n",
    "print(f\"  • LR scheduler: {training_args.lr_scheduler_type} (how LR changes)\")\n",
    "print(f\"  • Weight decay: {training_args.weight_decay} (regularization strength)\")\n",
    "print(f\"  • Max grad norm: {training_args.max_grad_norm} (gradient clipping)\")\n",
    "\n",
    "print(f\"\\n🔧 Fine-tuning tips:\")\n",
    "print(f\"  • Model not personalizing? → Increase epochs (7-10) or learning_rate (1e-3)\")\n",
    "print(f\"  • Training unstable? → Decrease learning_rate (1e-4) or increase warmup_steps (100)\")\n",
    "print(f\"  • Overfitting? → Increase weight_decay (0.05) or decrease epochs\")\n",
    "print(f\"  • GPU memory error? → Decrease per_device_train_batch_size to 1\")\n",
    "print(f\"  • Training too slow? → Increase per_device_train_batch_size or gradient_accumulation_steps\")\n",
    "\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")\n",
    "print(f\"Tokenizer pad token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb39ac9",
   "metadata": {},
   "source": [
    "## 🚀 Section 9: Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35643517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "print(\"This may take 15-30 minutes depending on your GPU.\\n\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9771b2",
   "metadata": {},
   "source": [
    "## 💾 Section 10: Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"✅ Model saved to {output_dir}\")\n",
    "\n",
    "# List saved files\n",
    "import os\n",
    "saved_files = os.listdir(output_dir)\n",
    "print(f\"\\n📁 Saved files: {saved_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb45b7b",
   "metadata": {},
   "source": [
    "## 🧹 Section 10.5: Memory Cleanup and Model Preparation\n",
    "\n",
    "Before testing the fine-tuned model, let's clear GPU memory and prepare the model for inference to avoid device placement errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e65fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache and optimize memory usage\n",
    "import gc\n",
    "\n",
    "print(\"🧹 Cleaning up memory before inference...\")\n",
    "\n",
    "# Clear PyTorch cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"✅ GPU memory cleared\")\n",
    "    print(f\"💾 GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"💾 GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Ensure model is in eval mode and properly on device\n",
    "peft_model.eval()\n",
    "print(\"✅ Model set to evaluation mode\")\n",
    "\n",
    "# Check and fix device placement for PEFT models\n",
    "target_device = device if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"🎯 Target device: {target_device}\")\n",
    "\n",
    "# Move the entire model to target device\n",
    "peft_model = peft_model.to(target_device)\n",
    "\n",
    "# Check all model components are on the correct device\n",
    "print(f\"📍 Model device: {next(peft_model.parameters()).device}\")\n",
    "print(f\"📍 Model dtype: {next(peft_model.parameters()).dtype}\")\n",
    "\n",
    "# Additional device checks for PEFT models\n",
    "try:\n",
    "    # Check base model device\n",
    "    base_model_device = next(peft_model.base_model.parameters()).device\n",
    "    print(f\"📍 Base model device: {base_model_device}\")\n",
    "    \n",
    "    # Check if there are any parameters on different devices\n",
    "    devices = set()\n",
    "    for name, param in peft_model.named_parameters():\n",
    "        devices.add(param.device)\n",
    "    \n",
    "    print(f\"📍 All parameter devices: {devices}\")\n",
    "    \n",
    "    if len(devices) > 1:\n",
    "        print(\"⚠️  Warning: Model has parameters on multiple devices!\")\n",
    "        print(\"🔧 Attempting to consolidate to single device...\")\n",
    "        peft_model = peft_model.to(target_device)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not check all device placements: {e}\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"🔍 Final model device check: {next(peft_model.parameters()).device}\")\n",
    "print(f\"🔍 Tokenizer device compatibility: tokenizer works with any device\")\n",
    "\n",
    "print(\"\\n🎯 Ready for inference testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249d3d1",
   "metadata": {},
   "source": [
    "## 🧪 Section 11: Compare Base Model vs Fine-tuned Model\n",
    "\n",
    "Let's test both the original base model and your fine-tuned LoRA model with the same prompts to see the difference in personalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b95cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load a separate base model without quantization for comparison\n",
    "print(\"🔄 Loading separate base model for comparison (without quantization)...\")\n",
    "\n",
    "# Load a clean base model without 8-bit quantization for comparison\n",
    "base_model_for_comparison = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=False,  # No quantization for the comparison model\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ Base model loaded successfully for comparison\")\n",
    "\n",
    "class BaseChatbot:\n",
    "    \"\"\"Interface for chatting with the original base model (no fine-tuning)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # Get the device of the model\n",
    "        self.device = next(model.parameters()).device\n",
    "        print(f\"💡 Base Chatbot initialized on device: {self.device}\")\n",
    "        \n",
    "        # For non-quantized models, we can safely move to device\n",
    "        # Don't move 8-bit models as they're already properly placed\n",
    "        if not getattr(model, 'is_loaded_in_8bit', False):\n",
    "            self.model = self.model.to(self.device)\n",
    "            print(f\"📍 Base Model moved to device: {next(self.model.parameters()).device}\")\n",
    "        else:\n",
    "            print(f\"📍 Base Model (8-bit) already on device: {next(self.model.parameters()).device}\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_length: int = 200, temperature: float = 0.8) -> str:\n",
    "        \"\"\"Generate a response using the base model (no personalization)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Format the prompt using the same template\n",
    "            formatted_prompt = f\"<|system|>\\nYou are a helpful assistant that knows about John Doe's personality, preferences, and experiences.</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n",
    "            \n",
    "            # Tokenize and move to device\n",
    "            inputs = self.tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(self.device)\n",
    "            \n",
    "            # Generate with the base model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_length=len(inputs[0]) + max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=50,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                    num_return_sequences=1,\n",
    "                    repetition_penalty=1.15,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    # Remove early_stopping to avoid the warning\n",
    "                )\n",
    "            \n",
    "            # Decode the response\n",
    "            outputs = outputs.cpu()\n",
    "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract just the assistant's response\n",
    "            if \"<|assistant|>\" in full_response:\n",
    "                response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "            else:\n",
    "                response = full_response[len(formatted_prompt):].strip()\n",
    "            \n",
    "            # Clean up the response\n",
    "            response = response.replace(\"</s>\", \"\").strip()\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Base model error: {str(e)}\"\n",
    "\n",
    "class PersonalChatbot:\n",
    "    \"\"\"Interface for chatting with the fine-tuned LoRA model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # Get the device of the model\n",
    "        self.device = next(model.parameters()).device\n",
    "        print(f\"💡 LoRA Chatbot initialized on device: {self.device}\")\n",
    "        \n",
    "        # For PEFT models with 8-bit base models, don't use .to() method\n",
    "        # The model is already properly placed by PEFT\n",
    "        if not getattr(model.base_model.model, 'is_loaded_in_8bit', False):\n",
    "            self.model = self.model.to(self.device)\n",
    "            print(f\"📍 LoRA Model moved to device: {next(self.model.parameters()).device}\")\n",
    "        else:\n",
    "            print(f\"📍 LoRA Model (8-bit base) already on device: {next(self.model.parameters()).device}\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_length: int = 200, temperature: float = 0.8) -> str:\n",
    "        \"\"\"Generate a response to a user prompt with optimized parameters\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Format the prompt using the same template as training\n",
    "            formatted_prompt = f\"<|system|>\\nYou are a helpful assistant that knows about John Doe's personality, preferences, and experiences.</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\\n\"\n",
    "            \n",
    "            # Tokenize with explicit device placement\n",
    "            inputs = self.tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
    "            \n",
    "            # Move inputs to the same device as model\n",
    "            inputs = inputs.to(self.device)\n",
    "            \n",
    "            # Generate with optimized parameters for better personalization\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_length=len(inputs[0]) + max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=50,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                    num_return_sequences=1,\n",
    "                    repetition_penalty=1.15,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    # Remove early_stopping to avoid the warning\n",
    "                )\n",
    "            \n",
    "            # Move outputs to CPU for decoding\n",
    "            outputs = outputs.cpu()\n",
    "            \n",
    "            # Decode the response\n",
    "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract just the assistant's response\n",
    "            if \"<|assistant|>\" in full_response:\n",
    "                response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "            else:\n",
    "                response = full_response[len(formatted_prompt):].strip()\n",
    "            \n",
    "            # Clean up the response\n",
    "            response = response.replace(\"</s>\", \"\").strip()\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Try alternative approach with explicit tensor handling\n",
    "            try:\n",
    "                # Use tokenizer with return_tensors and explicit device\n",
    "                encoding = self.tokenizer(\n",
    "                    formatted_prompt, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                )\n",
    "                \n",
    "                # Move all tensors in encoding to device\n",
    "                for key in encoding:\n",
    "                    if isinstance(encoding[key], torch.Tensor):\n",
    "                        encoding[key] = encoding[key].to(self.device)\n",
    "                \n",
    "                # Generate with explicit input_ids and attention_mask\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=encoding['input_ids'],\n",
    "                        attention_mask=encoding.get('attention_mask', None),\n",
    "                        max_length=encoding['input_ids'].shape[1] + max_length,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        top_p=0.9,\n",
    "                        top_k=50,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        use_cache=True,\n",
    "                        repetition_penalty=1.15,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        # Remove early_stopping to avoid the warning\n",
    "                    )\n",
    "                \n",
    "                # Decode response\n",
    "                outputs = outputs.cpu()\n",
    "                full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                if \"<|assistant|>\" in full_response:\n",
    "                    response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "                else:\n",
    "                    response = full_response[len(formatted_prompt):].strip()\n",
    "                \n",
    "                response = response.replace(\"</s>\", \"\").strip()\n",
    "                \n",
    "                return response\n",
    "                \n",
    "            except Exception as e2:\n",
    "                return f\"LoRA model error: {str(e2)}\"\n",
    "\n",
    "# Create both chatbot instances\n",
    "print(\"🤖 Creating chatbot instances for comparison...\")\n",
    "\n",
    "# Base model chatbot (using the non-quantized version for comparison)\n",
    "try:\n",
    "    base_chatbot = BaseChatbot(base_model_for_comparison, tokenizer)\n",
    "    print(\"✅ Base model chatbot created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create base chatbot: {e}\")\n",
    "    # Fallback: try using the original quantized model but handle device placement carefully\n",
    "    print(\"🔄 Trying fallback with original model...\")\n",
    "    try:\n",
    "        # Create a wrapper that doesn't move the model\n",
    "        class QuantizedBaseChatbot(BaseChatbot):\n",
    "            def __init__(self, model, tokenizer):\n",
    "                self.model = model\n",
    "                self.tokenizer = tokenizer\n",
    "                self.device = next(model.parameters()).device\n",
    "                print(f\"💡 Quantized Base Chatbot initialized on device: {self.device}\")\n",
    "                # Don't try to move 8-bit models\n",
    "        \n",
    "        base_chatbot = QuantizedBaseChatbot(model, tokenizer)\n",
    "        print(\"✅ Fallback base model chatbot created successfully\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Fallback also failed: {e2}\")\n",
    "        base_chatbot = None\n",
    "\n",
    "# LoRA fine-tuned model chatbot\n",
    "try:\n",
    "    lora_chatbot = PersonalChatbot(peft_model, tokenizer)\n",
    "    print(\"✅ LoRA model chatbot created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create LoRA chatbot: {e}\")\n",
    "    lora_chatbot = None\n",
    "\n",
    "# Test prompts designed to show personalization differences\n",
    "test_prompts = [\n",
    "    \"What is John's favorite weekend activity?\",\n",
    "    \"What does John think about artificial intelligence?\", \n",
    "    \"How does John handle stress?\",\n",
    "    \"What kind of books does John like to read?\",\n",
    "    \"What's John's morning routine like?\",\n",
    "    \"How does John use social media?\",\n",
    "    \"What does John do when he feels stuck or uninspired?\",\n",
    "    \"Tell me about John's perspective on work-life balance\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🔍 MODEL COMPARISON: BASE vs FINE-TUNED\")\n",
    "print(\"=\"*100)\n",
    "print(\"This comparison will show how fine-tuning affects the model's responses.\")\n",
    "print(\"Look for differences in personalization and specific details about John.\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Only run comparison if both chatbots were created successfully\n",
    "if base_chatbot is not None and lora_chatbot is not None:\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\n{'='*20} PROMPT {i} {'='*20}\")\n",
    "        print(f\"🤔 Question: {prompt}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        # Get base model response\n",
    "        print(\"📰 BASE MODEL (Original TinyLlama):\")\n",
    "        try:\n",
    "            base_response = base_chatbot.generate_response(prompt, max_length=150, temperature=0.8)\n",
    "            print(f\"   {base_response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Base model error: {e}\")\n",
    "        \n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        # Get LoRA model response\n",
    "        print(\"✨ FINE-TUNED MODEL (After LoRA training):\")\n",
    "        try:\n",
    "            lora_response = lora_chatbot.generate_response(prompt, max_length=150, temperature=0.8)\n",
    "            print(f\"   {lora_response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ LoRA model error: {e}\")\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(\"❌ Could not create both chatbots for comparison.\")\n",
    "    print(\"💡 You can still test the fine-tuned model individually if it was created successfully.\")\n",
    "    \n",
    "    if lora_chatbot is not None:\n",
    "        print(\"\\n🔍 TESTING FINE-TUNED MODEL ONLY:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts[:3], 1):  # Test first 3 prompts\n",
    "            print(f\"\\n📝 Prompt {i}: {prompt}\")\n",
    "            print(\"✨ Fine-tuned Response:\")\n",
    "            try:\n",
    "                response = lora_chatbot.generate_response(prompt, max_length=150, temperature=0.8)\n",
    "                print(f\"   {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "print(\"\\n🎯 WHAT TO LOOK FOR IN THE COMPARISON:\")\n",
    "print(\"📊 Base Model typically shows:\")\n",
    "print(\"   • Generic responses about AI assistants\")\n",
    "print(\"   • General advice without personal details\")\n",
    "print(\"   • No mention of John's specific interests (hiking, reading, etc.)\")\n",
    "print(\"   • Standard AI assistant language patterns\")\n",
    "\n",
    "print(\"\\n✨ Fine-tuned Model should show:\")\n",
    "print(\"   • Specific mentions of John's hiking hobby\")\n",
    "print(\"   • References to books like 'Sapiens' or '1984'\")\n",
    "print(\"   • John's coffee and reading routine\")\n",
    "print(\"   • His thoughtful, nature-loving personality\")\n",
    "print(\"   • Specific details from training data\")\n",
    "\n",
    "print(\"\\n📈 SUCCESS INDICATORS:\")\n",
    "print(\"   ✅ LoRA model mentions specific details from training data\")\n",
    "print(\"   ✅ Responses feel more personal and consistent\") \n",
    "print(\"   ✅ References to hiking, books, meditation, etc.\")\n",
    "print(\"   ✅ Different tone/style compared to base model\")\n",
    "\n",
    "print(\"\\n📊 If both models give similar responses:\")\n",
    "print(\"   🔄 Training may need more epochs (try 7-10)\")\n",
    "print(\"   📈 Increase learning rate (try 1e-3)\")\n",
    "print(\"   ⚙️  Increase LoRA alpha (try 128)\")\n",
    "print(\"   📚 Add more training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438dfd8",
   "metadata": {},
   "source": [
    "## 🎨 Section 12: Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b850f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface(base_chatbot: BaseChatbot, lora_chatbot: PersonalChatbot):\n",
    "    \"\"\"Create a Gradio web interface comparing both models\"\"\"\n",
    "    \n",
    "    def compare_models(message, base_history, lora_history):\n",
    "        # Get responses from both models\n",
    "        base_response = base_chatbot.generate_response(message)\n",
    "        lora_response = lora_chatbot.generate_response(message)\n",
    "        \n",
    "        # Update chat histories\n",
    "        base_history.append([message, base_response])\n",
    "        lora_history.append([message, lora_response])\n",
    "        \n",
    "        return \"\", base_history, lora_history\n",
    "    \n",
    "    def single_model_chat(message, history, use_lora=True):\n",
    "        if use_lora:\n",
    "            response = lora_chatbot.generate_response(message)\n",
    "        else:\n",
    "            response = base_chatbot.generate_response(message)\n",
    "        history.append([message, response])\n",
    "        return \"\", history\n",
    "    \n",
    "    with gr.Blocks(title=\"TinyLlama Personal Assistant Comparison\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# 🤖 TinyLlama Model Comparison: Base vs Fine-tuned\")\n",
    "        gr.Markdown(\"Compare responses between the original model and your personalized fine-tuned version!\")\n",
    "        \n",
    "        with gr.Tab(\"🔍 Side-by-Side Comparison\"):\n",
    "            gr.Markdown(\"### See how fine-tuning changes the model's responses\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"#### 📰 Base Model (Original)\")\n",
    "                    base_chatbot_interface = gr.Chatbot(height=400, label=\"Base TinyLlama\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"#### ✨ Fine-tuned Model (Personalized)\")\n",
    "                    lora_chatbot_interface = gr.Chatbot(height=400, label=\"John's Personal AI\")\n",
    "            \n",
    "            compare_msg = gr.Textbox(placeholder=\"Ask about John's preferences, habits, or personality...\", label=\"Your Question\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                compare_btn = gr.Button(\"🔍 Compare Both Models\", variant=\"primary\")\n",
    "                clear_compare_btn = gr.Button(\"Clear Both Chats\")\n",
    "            \n",
    "            # Comparison event handlers\n",
    "            compare_msg.submit(\n",
    "                compare_models, \n",
    "                [compare_msg, base_chatbot_interface, lora_chatbot_interface], \n",
    "                [compare_msg, base_chatbot_interface, lora_chatbot_interface]\n",
    "            )\n",
    "            compare_btn.click(\n",
    "                compare_models, \n",
    "                [compare_msg, base_chatbot_interface, lora_chatbot_interface], \n",
    "                [compare_msg, base_chatbot_interface, lora_chatbot_interface]\n",
    "            )\n",
    "            clear_compare_btn.click(\n",
    "                lambda: ([], [], \"\"), \n",
    "                outputs=[base_chatbot_interface, lora_chatbot_interface, compare_msg]\n",
    "            )\n",
    "            \n",
    "            # Example prompts for comparison\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    \"What is John's favorite weekend activity?\",\n",
    "                    \"How does John handle stress?\",\n",
    "                    \"What does John think about AI?\",\n",
    "                    \"What kind of books does John enjoy?\",\n",
    "                    \"What's John's morning routine?\",\n",
    "                    \"How does John use social media?\",\n",
    "                    \"What does John do when feeling stuck?\"\n",
    "                ],\n",
    "                inputs=compare_msg,\n",
    "                label=\"Try these comparison questions:\"\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"💬 Chat with Fine-tuned Model\"):\n",
    "            gr.Markdown(\"### Chat exclusively with your personalized John Doe AI\")\n",
    "            \n",
    "            lora_only_chat = gr.Chatbot(height=400, label=\"John's Personal AI\")\n",
    "            lora_msg = gr.Textbox(placeholder=\"Chat with the fine-tuned model...\", label=\"Your Message\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                lora_send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                lora_clear_btn = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            # Fine-tuned model only event handlers\n",
    "            lora_msg.submit(\n",
    "                lambda msg, hist: single_model_chat(msg, hist, use_lora=True), \n",
    "                [lora_msg, lora_only_chat], \n",
    "                [lora_msg, lora_only_chat]\n",
    "            )\n",
    "            lora_send_btn.click(\n",
    "                lambda msg, hist: single_model_chat(msg, hist, use_lora=True), \n",
    "                [lora_msg, lora_only_chat], \n",
    "                [lora_msg, lora_only_chat]\n",
    "            )\n",
    "            lora_clear_btn.click(lambda: ([], \"\"), outputs=[lora_only_chat, lora_msg])\n",
    "        \n",
    "        with gr.Tab(\"📊 About This Comparison\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## 🎯 What to Look For:\n",
    "            \n",
    "            ### \udcf0 Base Model Characteristics:\n",
    "            - Generic responses about AI capabilities\n",
    "            - General advice without personal context\n",
    "            - No specific mentions of John's interests\n",
    "            - Standard AI assistant language patterns\n",
    "            \n",
    "            ### ✨ Fine-tuned Model Improvements:\n",
    "            - **Specific details**: Mentions of hiking, reading specific books like 'Sapiens'\n",
    "            - **Personal habits**: Morning routines, coffee preferences, meditation\n",
    "            - **Personality traits**: Thoughtful, nature-loving, curious about learning\n",
    "            - **Consistent character**: Responses align with John's personality profile\n",
    "            \n",
    "            ## 📈 Training Success Indicators:\n",
    "            ✅ **Excellent**: LoRA model gives detailed, personalized responses with specific references  \n",
    "            ✅ **Good**: Some personalization visible, mentions general interests  \n",
    "            ⚠️ **Needs work**: Both models give similar generic responses  \n",
    "            \n",
    "            ## 🔧 If Models Are Too Similar:\n",
    "            - Increase training epochs (7-10)\n",
    "            - Higher learning rate (1e-3)\n",
    "            - Increase LoRA alpha (128)\n",
    "            - Add more training examples\n",
    "            \n",
    "            ## 📊 Your Training Results:\n",
    "            With a final loss of **0.155**, your model should show **excellent personalization**!\n",
    "            \"\"\")\n",
    "        \n",
    "        gr.Markdown(\"---\")\n",
    "        gr.Markdown(\"💡 **Training Info**: Model fine-tuned with LoRA on 30 synthetic examples about John Doe's personality, achieving loss of 0.155\")\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Create and launch the comparison interface\n",
    "print(\"🎨 Creating comprehensive Gradio comparison interface...\")\n",
    "demo = create_gradio_interface(base_chatbot, lora_chatbot)\n",
    "\n",
    "# Launch with public link\n",
    "print(\"🚀 Launching Gradio comparison interface...\")\n",
    "demo.launch(share=True, debug=True)\n",
    "\n",
    "print(\"\\n✅ Gradio comparison interface is now running!\")\n",
    "print(\"🔍 Use the 'Side-by-Side Comparison' tab to see the difference between models\")\n",
    "print(\"💬 Use the 'Chat with Fine-tuned Model' tab for regular conversation\")\n",
    "print(\"📊 Check the 'About This Comparison' tab for interpretation guidance\")\n",
    "print(\"\\nClick the public link above to access your model comparison from anywhere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0342ba",
   "metadata": {},
   "source": [
    "## 🎯 Interactive Testing Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing - run this cell to test custom prompts with both models\n",
    "print(\"💬 Interactive Testing\")\n",
    "\n",
    "# Check which chatbots are available\n",
    "available_options = []\n",
    "if 'base_chatbot' in globals() and base_chatbot is not None:\n",
    "    available_options.append(\"1. Base model (original TinyLlama)\")\n",
    "if 'lora_chatbot' in globals() and lora_chatbot is not None:\n",
    "    available_options.append(\"2. Fine-tuned model (personalized)\")\n",
    "if len(available_options) == 2:\n",
    "    available_options.append(\"3. Compare both models\")\n",
    "\n",
    "if not available_options:\n",
    "    print(\"❌ No chatbots available. Please run Section 25 first to create the chatbot instances.\")\n",
    "else:\n",
    "    print(\"Choose which model to test:\")\n",
    "    for option in available_options:\n",
    "        print(option)\n",
    "    print(\"Enter your choice, then ask questions (type 'quit' to stop):\\n\")\n",
    "\n",
    "    model_choice = input(\"Choose model: \").strip()\n",
    "\n",
    "    if model_choice == \"1\" and 'base_chatbot' in globals() and base_chatbot is not None:\n",
    "        print(\"🔄 Using Base Model\")\n",
    "        active_chatbot = base_chatbot\n",
    "        model_name = \"Base TinyLlama\"\n",
    "    elif model_choice == \"2\" and 'lora_chatbot' in globals() and lora_chatbot is not None:\n",
    "        print(\"✨ Using Fine-tuned Model\")\n",
    "        active_chatbot = lora_chatbot\n",
    "        model_name = \"John's Personal AI\"\n",
    "    elif model_choice == \"3\" and 'base_chatbot' in globals() and base_chatbot is not None and 'lora_chatbot' in globals() and lora_chatbot is not None:\n",
    "        print(\"🔍 Comparing Both Models\")\n",
    "        active_chatbot = None\n",
    "        model_name = \"Both Models\"\n",
    "    else:\n",
    "        # Default to available chatbot\n",
    "        if 'lora_chatbot' in globals() and lora_chatbot is not None:\n",
    "            print(\"✨ Defaulting to Fine-tuned Model\")\n",
    "            active_chatbot = lora_chatbot\n",
    "            model_name = \"John's Personal AI\"\n",
    "        elif 'base_chatbot' in globals() and base_chatbot is not None:\n",
    "            print(\"🔄 Defaulting to Base Model\")\n",
    "            active_chatbot = base_chatbot\n",
    "            model_name = \"Base TinyLlama\"\n",
    "        else:\n",
    "            print(\"❌ No chatbots available!\")\n",
    "            active_chatbot = None\n",
    "\n",
    "    if active_chatbot is not None or model_choice == \"3\":\n",
    "        print(f\"\\n💬 Interactive Testing with {model_name}\")\n",
    "        print(\"Enter your questions below (type 'quit' to stop):\\n\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'stop']:\n",
    "                print(\"Goodbye! 👋\")\n",
    "                break\n",
    "            \n",
    "            if user_input:\n",
    "                if model_choice == \"3\" and 'base_chatbot' in globals() and base_chatbot is not None and 'lora_chatbot' in globals() and lora_chatbot is not None:\n",
    "                    # Compare both models\n",
    "                    print(f\"\\n📰 Base Model Response:\")\n",
    "                    try:\n",
    "                        base_response = base_chatbot.generate_response(user_input)\n",
    "                        print(f\"   {base_response}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ❌ Error: {e}\")\n",
    "                    \n",
    "                    print(f\"\\n✨ Fine-tuned Model Response:\")\n",
    "                    try:\n",
    "                        lora_response = lora_chatbot.generate_response(user_input)\n",
    "                        print(f\"   {lora_response}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ❌ Error: {e}\")\n",
    "                    print()\n",
    "                else:  # Single model\n",
    "                    try:\n",
    "                        response = active_chatbot.generate_response(user_input)\n",
    "                        print(f\"{model_name}: {response}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error: {e}\\n\")\n",
    "    else:\n",
    "        print(\"❌ No chatbots available for testing. Please run Section 25 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b0edd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- ✅ Generated synthetic personal data for John Doe\n",
    "- ✅ Fine-tuned TinyLlama using LoRA for memory efficiency\n",
    "- ✅ Created an interactive chatbot that knows about John's personality\n",
    "- ✅ Built a Gradio web interface for easy interaction\n",
    "\n",
    "### Next Steps:\n",
    "1. **Expand the dataset**: Add more categories of personal data\n",
    "2. **Experiment with parameters**: Try different LoRA configurations\n",
    "3. **Add evaluation**: Create metrics to measure personalization quality\n",
    "4. **Deploy**: Host your model on Hugging Face Spaces or other platforms\n",
    "\n",
    "### 🔧 Customization Ideas:\n",
    "- Change the person's name and characteristics\n",
    "- Add new data categories (work history, family, hobbies)\n",
    "- Fine-tune on your own personal data (with privacy considerations)\n",
    "- Experiment with different base models\n",
    "\n",
    "**Happy fine-tuning!** 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
